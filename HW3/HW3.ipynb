{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv2wdgDGOJcr"
   },
   "source": [
    "## Fall 2024 CS4641/CS7641 Homework 3\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Friday, November 8th, 11:59 pm EST\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "\n",
    "- No unapproved extension of the deadline is allowed. Submission past our 48-hour penalized acceptance period will lead to 0 credit.\n",
    "\n",
    "- Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "  <font color='darkred'>\n",
    "- Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own.</font>\n",
    "  <font color='darkred'>\n",
    "- All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the institute’s Academic Integrity procedures. If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, **WE WILL DIRECTLY REPORT ALL CASES TO OSI**, which may, unfortunately, lead to a very harsh outcome. **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A56xiji5OJct"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- Unless a theory question explicitly states that no work is required to be shown, you must provide an explanation, justification, or calculation for your answer.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "- You can directly type Latex equations into markdown cells.\n",
    "- If a question requires a picture, you could use this syntax `<img src=\"\" style=\"width: 300px;\"/>` to include them within your ipython notebook.\n",
    "\n",
    "- Your write-up must be submitted in PDF format. Please ensure all questions are answered within the Jupyter Notebook using either Markdown or LaTeX. <font color = 'darkred'>We will **NOT** accept handwritten work. </font> Make sure that your work is formatted correctly, for example submit $\\sum_{i=0} x_i$ instead of \\text{sum\\_\\{i=0\\} x_i}\n",
    "- When submitting the non-programming part of your assignment, you must correctly map pages of your PDF to each question/subquestion to reflect where they appear. <font color='darkred'>**Improperly mapped questions may not be graded correctly and/or will result in point deductions for the error.**</font>\n",
    "- All assignments should be done individually, and each student must write up and submit their own answers.\n",
    "- **Graduate Students**: You are required to complete any sections marked as Bonus for Undergrads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the autograder\n",
    "\n",
    "- Grads will find three assignments on Gradescope that correspond to HW3: \"Assignment 3 Programming\", \"Assignment 3 - Non-programming\" and \"Assignment 3 Programming - Bonus for all\". Undergrads will find an additional assignment called \"Assignment 3 Programming - Bonus for Undergrads\".\n",
    "<!-- No changes needed on the below section -->\n",
    "- You will submit your code for the autograder in the Assignment 3 Programming sections. Please refer to the Deliverables and Point Distribution section for what parts are considered required, bonus for undergrads, and bonus for all.\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "<!-- No changes needed on the above section -->\n",
    "\n",
    "- **For the \"Assignment 3 - Non-programming\" part, you will need to submit to Gradescope a PDF copy of your Jupyter Notebook with the cell outputs.** Please refer to the **Deliverables and Point Distribution** section for an outline of the non-programming questions.\n",
    "\n",
    "- **When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem. The pages in the PDF should be of size 8.5\" x 11\", otherwise there may be a deduction in points for extra long sheets.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the local tests <a id='using_local_tests'></a>\n",
    "\n",
    "- For some of the programming questions we have included a local test using a small toy dataset to aid in debugging. The local test sample data and outputs are stored in .py files in the **local_tests_folder**. The actual local tests are stored in **localtests.py**. Both can be found under the **utilities** folder.\n",
    "- There are no points associated with passing or failing the local tests, you must still pass the autograder to get points.\n",
    "- **It is possible to fail the local test and pass the autograder** since the autograder has a certain allowed error tolerance while the local test allowed error may be smaller. Likewise, passing the local tests does not guarantee passing the autograder.\n",
    "- **You do not need to pass both local and autograder tests to get points, passing the Gradescope autograder is sufficient for credit.**\n",
    "- It might be helpful to comment out the tests for functions that have not been completed yet.\n",
    "- It is recommended to test the functions as it gets completed instead of completing the whole class and then testing. This may help in isolating errors. Do not solely rely on the local tests, continue to test on the autograder regularly as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0d4OUW-OJcu"
   },
   "source": [
    "## Deliverables and Points Distribution\n",
    "\n",
    "### Q1: Image Compression [30pts]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>imgcompression.py</font>\n",
    "\n",
    "- **1.1 Image Compression** [30 pts] - _programming_\n",
    "\n",
    "  - svd [6pts]\n",
    "\n",
    "  - compress [6pts]\n",
    "\n",
    "  - rebuild_svd [6pts]\n",
    "\n",
    "  - compression_ratio [6pts]\n",
    "\n",
    "  - recovered_variance_proportion [6pts]\n",
    "\n",
    "- **1.2 Black and White Compression** [0 pts]\n",
    "\n",
    "- **1.3 Color Compression** [0 pts]\n",
    "\n",
    "### Q2: Understanding PCA [20pts]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>pca.py and Written portion</font>\n",
    "\n",
    "- **2.1 PCA Implementation** [10 pts] - _programming_\n",
    "\n",
    "  - fit [5pts]\n",
    "\n",
    "  - transform [2pts]\n",
    "\n",
    "  - transform_rv [3pts]\n",
    "\n",
    "- **2.2 Visualize** [5 pts] _non-programming_\n",
    "\n",
    "- **2.3 PCA Reduced Facemask Dataset Analysis** [5 pts] _non-programming_\n",
    "\n",
    "- **2.4 PCA Exploration** [0 pts]\n",
    "\n",
    "### Q3: Regression and Regularization [72pts: 52pts + 20pts Grad / 6% Bonus for Undergrads + 2.3% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>regression.py and Written portion</font>\n",
    "\n",
    "- **3.1 About RMSE** [3 pts] _non-programming_\n",
    "\n",
    "- **3.2 Regression and Regularization Implementations** [50pts: 30pts + 20pts Grad / 6% Bonus for Undergrad] - _programming_\n",
    "\n",
    "  - RMSE [5pts]\n",
    "\n",
    "  - Construct Poly Features 1D [2pts]\n",
    "\n",
    "  - Construct Poly Features 2D [3pts]\n",
    "\n",
    "  - Prediction [5pts]\n",
    "\n",
    "  - Linear Fit Closed Form [5pts]\n",
    "\n",
    "  - Ridge Fit Closed Form [5pts]\n",
    "\n",
    "  - Cross Validation [5pts]\n",
    "\n",
    "  - Linear Gradient Descent [5pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "  - Linear Stochastic Gradient Descent [5pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "  - Ridge Gradient Descent [5pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "  - Ridge Stochastic Gradient Descent [5pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "- **3.3 Testing: General Functions and Linear Regression** [5 pts] _programming_\n",
    "\n",
    "- **3.4 Testing: Ridge Regression** [5 pts] _programming_\n",
    "\n",
    "- **3.5 Linear vs. Ridge Regression Analysis** [4 pts] _non-programming_\n",
    "\n",
    "- **3.6 Cross Validation Hyperparameter Search** [5 pts] _programming_\n",
    "\n",
    "- **3.7 Noisy Input Samples in Linear Regression** [2.3% **BONUS FOR ALL**] _non-programming_ \n",
    "\n",
    "### Q4: Naive Bayes and Logistic Regression [39pts]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>logistic_regression.py and Written portion</font>\n",
    "\n",
    "- **4.1 Profile Screening Problem** [7 pts] _non-programming_\n",
    "  - Profile Screening Problem using Naive Bayes [5pts]\n",
    "\n",
    "  - AI-Driven Profile Screening [2pts]\n",
    "\n",
    "- **4.2 News Data Sentiment Classification Using Logistic Regression** [30 pts] - _programming_\n",
    "\n",
    "  - sigmoid [2 pts]\n",
    "\n",
    "  - bias_augment [3 pts]\n",
    "\n",
    "  - predict_probs [5 pts]\n",
    "\n",
    "  - predict_labels [2 pts]\n",
    "\n",
    "  - loss [3 pts]\n",
    "\n",
    "  - gradient [3 pts]\n",
    "\n",
    "  - accuracy [2 pts]\n",
    "\n",
    "  - evaluate [5 pts]\n",
    "\n",
    "  - fit [5 pts]\n",
    "\n",
    "- **4.3 Logistic Regression Threshold Experiments** [2 pts] _non-programming_\n",
    "\n",
    "### Q5: Feature Selection [30pts]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>feature_reduction.py</font>\n",
    "\n",
    "- **5.1 Feature Reduction** [30pts] - _programming_\n",
    "\n",
    "  - forward_selection [15pts]\n",
    "\n",
    "  - backward_elimination [15pts]\n",
    "\n",
    "### Q6: Imbalanced Classes in Classification Tasks [5.6% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>smote.py</font>\n",
    "\n",
    "- **6.1 A More Comprehensive Measure** [1.75%] _programming_\n",
    "\n",
    "  - generate_confusion_matrix [0.75%]\n",
    "\n",
    "  - f1_scores [1%]\n",
    "\n",
    "\n",
    "- **6.2 SMOTE** [3.85%] _programming_\n",
    "\n",
    "  - interpolate [1%]\n",
    "\n",
    "  - k_nearest_neighbors [0.75%]\n",
    "\n",
    "  - smote [2.1%]\n",
    "\n",
    "\n",
    "### Q7: Movie Recommendation with SVD [2.1% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>svd_recommender.py and Written portion (also include imgcompression.py in the submission as svd_recommender.py imports imgcompression)</font>\n",
    "\n",
    "- **7.1 SVD Recommender** [2.1%] _programming_\n",
    "\n",
    "  - recommender_svd [1.05%]\n",
    "\n",
    "  - predict [1.05%]\n",
    "\n",
    "- **7.2 Visualize Movie Vectors** [0pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points Totals: ###\n",
    "- Total Base: 191 pts for grads / 171 pts for undergrads\n",
    "- Total Undergrad Bonus: 6%\n",
    "- Total Bonus for All: 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Submit the following files to their respective assignments on Gradescope for the programming portions:\n",
    "\n",
    "- **Assignment 3 Programming**\n",
    "    - imgcompression.py\n",
    "    - pca.py\n",
    "    - regression.py\n",
    "    - logistic_regression.py\n",
    "    - feature_reduction.py\n",
    "    \n",
    "- **Assignment 3 Bonus for Undergrad - Programming**\n",
    "    - regression.py\n",
    "\n",
    "- **Assignment 3 Bonus for All - Programming**\n",
    "    - smote.py\n",
    "    - imgcompression.py (used as an import in svd_recommender.py)\n",
    "    - svd_recommender.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ7VM2hKOJcv"
   },
   "source": [
    "## 0 Set up\n",
    "\n",
    "<!--Update Python version and update links-->\n",
    "\n",
    "This notebook is tested under [python 3._ ._](https://www.python.org/downloads/release/python-3109/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "- [matplotlib](https://matplotlib.org/stable/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py)\n",
    "- [sklearn](https://scikit-learn.org/stable/tutorial/index.html)\n",
    "\n",
    "There is also a [VS Code and Anaconda Setup Tutorial](https://edstem.org/us/courses/58807/discussion/4951393) on Ed under the \"Links\" category\n",
    "\n",
    "Please implement the functions that have `raise NotImplementedError`, and after you finish the coding, please delete or comment out `raise NotImplementedError`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Library imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T02:09:18.781471700Z",
     "start_time": "2023-10-18T02:09:18.449781900Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:15.522829Z",
     "iopub.status.busy": "2024-10-05T00:16:15.522512Z",
     "iopub.status.idle": "2024-10-05T00:16:20.410866Z",
     "shell.execute_reply": "2024-10-05T00:16:20.409446Z"
    },
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# This is cell which sets up some of the modules you might need\n",
    "# Please do not change the cell or import any additional packages.\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import (\n",
    "    load_breast_cancer,\n",
    "    load_diabetes,\n",
    "    load_iris,\n",
    "    make_classification,\n",
    ")\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"Version information\")\n",
    "\n",
    "print(\"python: {}\".format(sys.version))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"numpy: {}\".format(np.__version__))\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "STUDENT_VERSION = 0\n",
    "EO_TEXT, EO_FONT, EO_COLOR = (\n",
    "    \"TA VERSION\",\n",
    "    \"Chalkduster\",\n",
    "    \"gray\",\n",
    ")\n",
    "EO_ALPHA, EO_SIZE, EO_ROT = 0.7, 90, 40\n",
    "\n",
    "pio.renderers.default = \"notebook+vscode+png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Q1: Image Compression [30 pts] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images data and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:20.417284Z",
     "iopub.status.busy": "2024-10-05T00:16:20.416902Z",
     "iopub.status.idle": "2024-10-05T00:16:22.380735Z",
     "shell.execute_reply": "2024-10-05T00:16:22.379734Z"
    },
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# load Image\n",
    "image = plt.imread(\"./data/hw3_image_compression.jpeg\") / 255\n",
    "# plot image\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:22.407250Z",
     "iopub.status.busy": "2024-10-05T00:16:22.406898Z",
     "iopub.status.idle": "2024-10-05T00:16:23.141286Z",
     "shell.execute_reply": "2024-10-05T00:16:23.140269Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(rgb2gray(image), cmap=plt.cm.bone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Image compression [30pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "SVD is a dimensionality reduction technique that allows us to compress images by throwing away the least important information.\n",
    "\n",
    "Higher singular values capture greater variance and, thus, capture greater information from the corresponding singular vector. To perform image compression, apply SVD on each matrix and get rid of the small singular values to compress the image. The loss of information through this process is negligible, and the difference between the images can be hardly spotted.\n",
    "\n",
    "For example, the proportion of variance captured by the first component is $$\\frac{\\sigma_1^2}{\\sum_{i=1}^n \\sigma_i^2}$$ where $\\sigma_i$ is the $i^{th}$ singular value.\n",
    "\n",
    "In the <strong>imgcompression.py</strong> file, complete the following functions:\n",
    "\n",
    "- **svd**: You may use <samp>np.linalg.svd</samp> in this function, and although the function defaults this parameter to true, you may explicitly set <samp>full_matrices=True</samp> using the optional <samp>full_matrices</samp> parameter. Hint 2 may be useful.\n",
    "- **compress**\n",
    "- **rebuild_svd**\n",
    "- **compression_ratio**: Hint 1 may be useful\n",
    "- **recovered_variance_proportion**: Hint 1 may be useful\n",
    "\n",
    "**HINT 1:** http://timbaumann.info/svd-image-compression-demo/ is a useful article on image compression and compression ratio.\n",
    "\n",
    "**HINT 2:** If you have never used <samp>np.linalg.svd</samp>, it might be helpful to read [Numpy's SVD documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) and note the particularities of the $V$ matrix and that it is returned already transposed.\n",
    "\n",
    "**HINT 3:** The shape of $S$ resulting from SVD may change depending on if N > D, N < D, or N = D. Therefore, when checking the shape of $S$, note that min(N,D) means the value should be equal to whichever is lower between N and D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Local Tests for Image Compression Black and White Case [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **imgcompression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:23.174158Z",
     "iopub.status.busy": "2024-10-05T00:16:23.173846Z",
     "iopub.status.idle": "2024-10-05T00:16:23.519928Z",
     "shell.execute_reply": "2024-10-05T00:16:23.518719Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestImgCompression\n",
    "\n",
    "unittest_ic = TestImgCompression()\n",
    "unittest_ic.test_svd_bw()\n",
    "unittest_ic.test_compress_bw()\n",
    "unittest_ic.test_rebuild_svd_bw()\n",
    "unittest_ic.test_compression_ratio_bw()\n",
    "unittest_ic.test_recovered_variance_proportion_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Local Tests for Image Compression Color Case [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **imgcompression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n",
    "\n",
    "**HINT 1:** Make sure your implementation of <samp>recovered_variance_proportion</samp> returns an array of 3 floats for a color image.  \n",
    "**HINT 2:** Try performing SVD on the individual color channels and then stack the individual channel $U$, $S$, $V$ matrices.  \n",
    "**HINT 3:** To improve image processing functions, you may need to implement separate methods for handling color and grayscale images within the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:23.524027Z",
     "iopub.status.busy": "2024-10-05T00:16:23.523647Z",
     "iopub.status.idle": "2024-10-05T00:16:23.553033Z",
     "shell.execute_reply": "2024-10-05T00:16:23.551799Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestImgCompression\n",
    "\n",
    "unittest_ic = TestImgCompression()\n",
    "\n",
    "unittest_ic.test_svd_color()\n",
    "unittest_ic.test_compress_color()\n",
    "unittest_ic.test_rebuild_svd_color()\n",
    "unittest_ic.test_compression_ratio_color()\n",
    "unittest_ic.test_recovered_variance_proportion_color()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Black and White Compression [No Points]\n",
    "\n",
    "This question will utilize your implementation of the functions from Q1.1 to compare the byte size needed for representing the SVD decomposition of the original image versus the compressed image at various compression levels. You can directly execute the following cell without modifying it, provided that you have already implemented the functions in Q1.1\n",
    "\n",
    "**Running this cell is primarily for your own understanding of the compression process.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:23.557620Z",
     "iopub.status.busy": "2024-10-05T00:16:23.557225Z",
     "iopub.status.idle": "2024-10-05T00:16:28.729158Z",
     "shell.execute_reply": "2024-10-05T00:16:28.728387Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from imgcompression import ImgCompression\n",
    "\n",
    "imcompression = ImgCompression()\n",
    "bw_image = rgb2gray(image)\n",
    "U, S, V = imcompression.svd(bw_image)\n",
    "component_num = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "# plot several images\n",
    "i = 0\n",
    "for k in component_num:\n",
    "    U_compressed, S_compressed, V_compressed = imcompression.compress(U, S, V, k)\n",
    "    img_rebuild = imcompression.rebuild_svd(U_compressed, S_compressed, V_compressed)\n",
    "    c = np.around(imcompression.compression_ratio(bw_image, k), 4)\n",
    "    r = np.around(imcompression.recovered_variance_proportion(S, k), 3)\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img_rebuild, cmap=plt.cm.bone)\n",
    "    ax.set_title(f\"{k} Components\")\n",
    "    ax.set_xlabel(f\"Compression: {c},\\nRecovered Variance: {r}\")\n",
    "    i = i + 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:28.781201Z",
     "iopub.status.busy": "2024-10-05T00:16:28.778172Z",
     "iopub.status.idle": "2024-10-05T00:16:31.137337Z",
     "shell.execute_reply": "2024-10-05T00:16:31.136372Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from imgcompression import ImgCompression\n",
    "\n",
    "imcompression = ImgCompression()\n",
    "bw_image = rgb2gray(image)\n",
    "U, S, V = imcompression.svd(bw_image)\n",
    "\n",
    "component_num = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "# Compare memory savings for BW image\n",
    "for k in component_num:\n",
    "    og_bytes, comp_bytes, savings = imcompression.memory_savings(bw_image, U, S, V, k)\n",
    "    comp_ratio = og_bytes / comp_bytes\n",
    "    og_bytes = imcompression.nbytes_to_string(og_bytes)\n",
    "    comp_bytes = imcompression.nbytes_to_string(comp_bytes)\n",
    "    savings = imcompression.nbytes_to_string(savings)\n",
    "    print(\n",
    "        f\"{k} components: Original Image: {og_bytes} -> Compressed Image: {comp_bytes}, Savings: {savings}, Compression Ratio {comp_ratio:.1f}:1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Color Compression [No Points]\n",
    "\n",
    "This section will use your implementation of the functions from Q1.1 to generate a set of images compressed to\n",
    "different degrees. You can simply run the below cell without making any changes to it, assuming you have implemented the functions in Q1.1.\n",
    "\n",
    "**Running this cell is primarily for your own understanding of the compression process.**\n",
    "\n",
    "<b>NOTE:</b> You might get warning \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\" This warning is acceptable since some of the pixels may go above 1.0 while rebuilding. You should see similar images to original even with such clipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:31.140634Z",
     "iopub.status.busy": "2024-10-05T00:16:31.140326Z",
     "iopub.status.idle": "2024-10-05T00:16:47.265608Z",
     "shell.execute_reply": "2024-10-05T00:16:47.264373Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from imgcompression import ImgCompression\n",
    "\n",
    "imcompression = ImgCompression()\n",
    "image_rolled = np.moveaxis(image, -1, 0)\n",
    "U, S, V = imcompression.svd(image_rolled)\n",
    "\n",
    "component_num = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "# plot several images\n",
    "i = 0\n",
    "for k in component_num:\n",
    "    U_compressed, S_compressed, V_compressed = imcompression.compress(U, S, V, k)\n",
    "    img_rebuild = np.clip(\n",
    "        imcompression.rebuild_svd(U_compressed, S_compressed, V_compressed), 0, 1\n",
    "    )\n",
    "    img_rebuild = np.moveaxis(img_rebuild, 0, -1)\n",
    "    c = np.around(imcompression.compression_ratio(image_rolled, k), 4)\n",
    "    r = np.around(imcompression.recovered_variance_proportion(S, k), 3)\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img_rebuild)\n",
    "    ax.set_title(f\"{k} Components\")\n",
    "    ax.set_xlabel(\n",
    "        f\"Compression: {np.around(c,4)},\\nRecovered Variance:  R: {r[0]}  G: {r[1]}  B: {r[2]}\"\n",
    "    )\n",
    "    i = i + 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:47.354866Z",
     "iopub.status.busy": "2024-10-05T00:16:47.352452Z",
     "iopub.status.idle": "2024-10-05T00:16:54.212691Z",
     "shell.execute_reply": "2024-10-05T00:16:54.211459Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from imgcompression import ImgCompression\n",
    "\n",
    "imcompression = ImgCompression()\n",
    "U, S, V = imcompression.svd(image_rolled)\n",
    "\n",
    "component_num = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "# Compare the memory savings of the color image\n",
    "i = 0\n",
    "for k in component_num:\n",
    "    og_bytes, comp_bytes, savings = imcompression.memory_savings(\n",
    "        image_rolled, U, S, V, k\n",
    "    )\n",
    "    comp_ratio = og_bytes / comp_bytes\n",
    "    og_bytes = imcompression.nbytes_to_string(og_bytes)\n",
    "    comp_bytes = imcompression.nbytes_to_string(comp_bytes)\n",
    "    savings = imcompression.nbytes_to_string(savings)\n",
    "    print(\n",
    "        f\"{k} components: Original Image: {og_bytes} -> Compressed Image: {comp_bytes}, Savings: {savings}, Compression Ratio {comp_ratio:.1f}:1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Understanding PCA [20 pts] <span style=\"color:blue\">**[P]**</span> | <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is another dimensionality reduction technique that reduces dimensions or features while still preserving the maximum (or close-to) amount of information. This is useful when analyzing large datasets that contain a high number of dimensions or features that may be correlated. PCA aims to eliminate features that are highly correlated and only retain the important/uncorrelated ones that can describe most or all the variance in the data. This enables better interpretability and visualization of the multi-dimensional data. In this problem, we will investigate how PCA can be used to improve features for regression and classification tasks and how the data itself affects the behavior of PCA.\n",
    "\n",
    "Here, we will employ Singular Value Decomposition (SVD) for PCA. In PCA, we first center the data by subtracting the mean of each feature. SVD is well suited for this task since each singular value tells us the amount of variance captured in each component for a given matrix (e.g. image). Hence, we can use SVD to extract data only in directions with high variances using either a threshold of the amount of variance or the number of bases/components. Here, we will reduce the data to a set number of components.\n",
    "\n",
    "Recall from class that in PCA, we project the original matrix $X$ into new components, each one corresponding to an eigenvector of the covariance matrix $X^T X$. We know that SVD decomposes X into three matrices U, S, and V^T. We can find the SVD decomposition of X^T\\*X using the decomposition for X as follows:\n",
    "$$X^TX = (USV^T)^TUSV^T = (VS^TU^T)USV^T = V S^2 V^T $$\n",
    "\n",
    "This means two important things for us:\n",
    "\n",
    "- The matrix $V^T$, often referred to as the _right singular vectors_ of $X$, is equivalent to the _eigenvectors_ of $X^TX$.\n",
    "- $S^2$ is equivalent to the _eigenvalues_ of $X^TX$.\n",
    "\n",
    "So the first $n$-principal components are obtained by projecting $X$ by the first $n$ vectors from $V^T$. Similarly, $S^2$ gives a measure of the variance retained.\n",
    "\n",
    "### 2.1 Implementation [10 pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "Implement PCA. In the <strong>pca.py</strong> file, complete the following functions:\n",
    "\n",
    "- <strong>fit</strong>: You may use <samp>np.linalg.svd</samp>. Set <samp>full_matrices=False</samp>. Hint 1 may be useful.\n",
    "- <strong>transform</strong>\n",
    "- <strong>transform_rv</strong>: You may find <samp>np.cumsum</samp> helpful for this function.\n",
    "\n",
    "Assume a dataset is composed of N datapoints, each of which has D features with D < N. The dimension of our data would be D. However, it is possible that many of these dimensions contain redundant information. Each feature explains part of the variance in our dataset, and some features may explain more variance than others.\n",
    "\n",
    "**HINT 1:** Make sure you remember to first center your data by subtracting the mean of each feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Local Tests for PCA [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **pca.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:54.217209Z",
     "iopub.status.busy": "2024-10-05T00:16:54.216466Z",
     "iopub.status.idle": "2024-10-05T00:16:54.251057Z",
     "shell.execute_reply": "2024-10-05T00:16:54.249657Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestPCA\n",
    "\n",
    "unittest_pca = TestPCA()\n",
    "unittest_pca.test_pca()\n",
    "unittest_pca.test_transform()\n",
    "unittest_pca.test_transform_rv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "PCA is used to transform multivariate data tables into smaller sets so as to observe the hidden trends and variations in the data. It can also be used as a feature extractor for images. Here you will visualize two datasets using PCA, including Iris Dataset and Facemask Dataset.\n",
    "\n",
    "In the **pca.py**, complete the following function:\n",
    "\n",
    "- **visualize**: Use your implementation of PCA and reduce the datasets such that they contain only two features. Using [Plotly's Express](https://plotly.com/python/line-and-scatter/) make a 2D and 3D scatterplot of the data points using these features. Make sure to differentiate the data points according to their true labels using color. We recommend converting the data to a pandas dataframe before plotting. In addition, make a 2D scatterplot of two random features following the same procedure and examine the difference.\n",
    "\n",
    "The datasets have already been loaded for you in the subsequent cells.\n",
    "\n",
    "**NOTE:** Here, we won't be testing for accuracy. Even with correct implementations of PCA, the accuracy can differ from the TA solution. That is fine as long as the visualizations come out similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:54.256218Z",
     "iopub.status.busy": "2024-10-05T00:16:54.255166Z",
     "iopub.status.idle": "2024-10-05T00:16:56.002132Z",
     "shell.execute_reply": "2024-10-05T00:16:55.929805Z"
    },
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Use PCA for visualization of iris dataset\n",
    "\n",
    "from pca import PCA\n",
    "\n",
    "iris_data = load_iris(return_X_y=True)\n",
    "\n",
    "X = iris_data[0]\n",
    "y = iris_data[1]\n",
    "\n",
    "fig_title = \"Iris Dataset with Dimensionality Reduction\"\n",
    "PCA().visualize(X, y, fig_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 PCA Reduced Facemask Dataset Analysis [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "#### Facemask Dataset\n",
    "\n",
    "The masked and unmasked dataset is made up of grayscale images of human faces facing forward. Half of these images are faces that are completely unmasked, and the remaining images show half of the face covered with an artificially generated face mask. The images have already been preprocessed, they are also reduced to a small size of 64x64 pixels and then reshaped into a feature vector of 4096 pixels. Below is a sample of some of the images in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:56.147594Z",
     "iopub.status.busy": "2024-10-05T00:16:56.135387Z",
     "iopub.status.idle": "2024-10-05T00:16:56.818626Z",
     "shell.execute_reply": "2024-10-05T00:16:56.817687Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "X = np.load(\"./data/smallflat_64.npy\")\n",
    "y = np.load(\"./data/masked_labels.npy\").astype(\"int\")\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "for idx in [0, 1, 2, 150, 151, 152]:\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    image = (\n",
    "        np.rot90(X[idx].reshape(64, 64), k=1)\n",
    "        if idx % 2 == 1 and idx < 150\n",
    "        else X[idx].reshape(64, 64)\n",
    "    )\n",
    "    m_status = \"Unmasked\" if idx < 150 else \"Masked\"\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(f\"{m_status} Image at i = {idx}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:56.823398Z",
     "iopub.status.busy": "2024-10-05T00:16:56.822809Z",
     "iopub.status.idle": "2024-10-05T00:16:58.032373Z",
     "shell.execute_reply": "2024-10-05T00:16:58.031091Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Use PCA for visualization of masked and unmasked images\n",
    "\n",
    "X = np.load(\"./data/smallflat_64.npy\")\n",
    "y = np.load(\"./data/masked_labels.npy\")\n",
    "\n",
    "fig_title = \"Facemask Dataset Visualization with Dimensionality Reduction\"\n",
    "PCA().visualize(X, y, fig_title)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"*In this plot, the 0 points are unmasked images and the 1 points are masked images.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of this 2 dimensional plot, knowing that the original dataset was originally a set of flattened image vectors that had 4096 pixels/features?.\n",
    "\n",
    "1. Examine the 2-dimensional plot of the _facemask_ dataset reduced to 2 principal components. How might PCA help in dealing with noise, overfitting, and feature correlation in a high-dimensional dataset? Discuss scenarios where using PCA-reduced data could outperform a classifier trained on the original high-dimensional data. **(3 pts)**\n",
    "\n",
    "   **Answer** ...\n",
    "\n",
    "\n",
    "2. When applying PCA to reduce a high-dimensional dataset (e.g., 4096 features) to just 2 components, how might this impact the classifier’s ability to capture non-linear patterns in the data? Discuss how the trade-off between maximizing variance (PCA’s goal) and preserving class separability could affect classification performance. **(2 pts)**\n",
    "\n",
    "   **Answer** ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PCA Exploration [No Points]\n",
    "\n",
    "**Note** The accuracy can differ from the TA solution and this section is not graded.\n",
    "\n",
    "#### Emotion Dataset [No Points]\n",
    "\n",
    "Now you will use PCA on an actual real-world dataset. We will use your implementation of PCA function to reduce the dataset with 99% retained variance and use it to obtain the reduced features. On the reduced dataset, we will use logistic and linear regression to compare results between PCA and non-PCA datasets. Run the following cells to see how PCA works on regression and classification tasks.\n",
    "\n",
    "The first dataset we will use is an emotion dataset made up of grayscale images of human faces faces that are visibly happy and visibly sad. Note how Accuracy increases after reducing the number of features used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:58.038228Z",
     "iopub.status.busy": "2024-10-05T00:16:58.037929Z",
     "iopub.status.idle": "2024-10-05T00:16:58.329957Z",
     "shell.execute_reply": "2024-10-05T00:16:58.327668Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "X = np.load(\"./data/emotion_features.npy\")\n",
    "y = np.load(\"./data/emotion_labels.npy\").astype(\"int\")\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "for idx in [0, 1, 2, 150, 151, 152]:\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    image = (\n",
    "        np.rot90(X[idx].reshape(64, 64), k=1)\n",
    "        if idx % 2 == 1 and idx < 150\n",
    "        else X[idx].reshape(64, 64)\n",
    "    )\n",
    "    m_status = \"Unmasked\" if idx < 150 else \"Masked\"\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    m_status = \"Sad\" if idx < 150 else \"Happy\"\n",
    "    ax.set_title(f\"{m_status} Image at i = {idx}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:58.336332Z",
     "iopub.status.busy": "2024-10-05T00:16:58.335420Z",
     "iopub.status.idle": "2024-10-05T00:16:58.978311Z",
     "shell.execute_reply": "2024-10-05T00:16:58.977756Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "X = np.load(\"./data/emotion_features.npy\")\n",
    "y = np.load(\"./data/emotion_labels.npy\").astype(\"int\")\n",
    "\n",
    "print(\"Not Graded - Data shape before PCA \", X.shape)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform_rv(X, retained_variance=0.99)\n",
    "\n",
    "print(\"Not Graded - Data shape with PCA \", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:16:58.982139Z",
     "iopub.status.busy": "2024-10-05T00:16:58.981416Z",
     "iopub.status.idle": "2024-10-05T00:17:00.669464Z",
     "shell.execute_reply": "2024-10-05T00:17:00.668406Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Use logistic regression to predict classes for test set\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print(\n",
    "    \"Not Graded - Accuracy before PCA: {:.5f}\".format(\n",
    "        accuracy_score(y_test, preds.argmax(axis=1))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:00.676205Z",
     "iopub.status.busy": "2024-10-05T00:17:00.675418Z",
     "iopub.status.idle": "2024-10-05T00:17:00.799957Z",
     "shell.execute_reply": "2024-10-05T00:17:00.798827Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Use logistic regression to predict classes for test set\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print(\n",
    "    \"Not Graded - Accuracy after PCA: {:.5f}\".format(\n",
    "        accuracy_score(y_test, preds.argmax(axis=1))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore sklearn's Diabetes dataset using PCA dimensionality reduction and regression. Notice the RMSE score reduction after we apply PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:00.809038Z",
     "iopub.status.busy": "2024-10-05T00:17:00.806581Z",
     "iopub.status.idle": "2024-10-05T00:17:00.849646Z",
     "shell.execute_reply": "2024-10-05T00:17:00.848995Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "\n",
    "def apply_regression(X_train, y_train, X_test):\n",
    "    ridge = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "    clf = ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:00.856999Z",
     "iopub.status.busy": "2024-10-05T00:17:00.854605Z",
     "iopub.status.idle": "2024-10-05T00:17:00.911951Z",
     "shell.execute_reply": "2024-10-05T00:17:00.910524Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# load the dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform_rv(X, retained_variance=0.9)\n",
    "print(\"Not Graded - data shape with PCA \", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:00.915915Z",
     "iopub.status.busy": "2024-10-05T00:17:00.915232Z",
     "iopub.status.idle": "2024-10-05T00:17:00.961138Z",
     "shell.execute_reply": "2024-10-05T00:17:00.960116Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Ridge regression without PCA\n",
    "y_pred = apply_regression(X_train, y_train, X_test)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print(\n",
    "    \"Not Graded - RMSE score using Ridge Regression before PCA: {:.5}\".format(\n",
    "        rmse_score\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:00.966628Z",
     "iopub.status.busy": "2024-10-05T00:17:00.966111Z",
     "iopub.status.idle": "2024-10-05T00:17:01.035908Z",
     "shell.execute_reply": "2024-10-05T00:17:01.034888Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Ridge regression with PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# use Ridge Regression for getting predicted labels\n",
    "y_pred = apply_regression(X_train, y_train, X_test)\n",
    "\n",
    "# calculate RMSE\n",
    "rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print(\n",
    "    \"Not Graded - RMSE score using Ridge Regression after PCA: {:.5}\".format(rmse_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8RXOTa_lpYN"
   },
   "source": [
    "## Q3 Polynomial regression and regularization [72pts: 52pts + 20pts Grad / 6% Bonus for Undergrads + 2.3% Bonus for All] <span style=\"color:blue\">**[P]**</span> | <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 About RMSE (Root Mean Square Error) [3 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Mean Squared Error (MSE) is used to provide an indication of how well a model is performing in terms of prediction accuracy.\n",
    "\n",
    "$$\n",
    "    MSE = \\frac{1}{n} \\sum_{i = 1}^{n} ( y_{i} - \\hat{y_{i}})^{2}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of data points\n",
    "- $y_{i}$ is the actual target value\n",
    "- $\\hat{y_{i}}$ is the predicted value by the model\n",
    "\n",
    "However, Root Mean Square Error (RMSE) is preferred over MSE because it brings the error back to the same units as the target variable, providing easier comparison to the actual values. \n",
    "\n",
    "This helps in providing better practical interpretation of how well the model is performing on both small and large errors.\n",
    "\n",
    "$$\n",
    "    RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} ( y_{i} - \\hat{y_{i}})^{2}}\n",
    "$$\n",
    "\n",
    "**What is a good RMSE value?**\n",
    "\n",
    "If we normalize our labels such that the true labels $y$ and the model outputs $\\hat{y}$ can only be between 0 and 1, what does it mean when the RMSE = 1? Please provide a toy example with your explanation.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cq4yJhTlpYN",
    "tags": []
   },
   "source": [
    "### 3.2 Regression and regularization implementations [50pts: 30pts + 20pts Grad / 6% Bonus for Undergrad] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "We have three methods to fit linear and ridge regression models: 1) closed form solution; 2) gradient descent (GD); 3) stochastic gradient descent (SGD). Some of the functions are bonus, see the below function list on what is required to be implemented for graduate and undergraduate students. We use the term weight in the following code. Weights and parameters ($\\theta$) have the same meaning here. We used parameters ($\\theta$) in the lecture slides.\n",
    "\n",
    "In the <strong>regression.py</strong> file, complete the Regression class by implementing the listed functions below. We have provided the Loss function, $L$, associated with the GD and SGD function for Linear and Ridge Regression for deriving the gradient update.\n",
    "\n",
    "- **rmse**\n",
    "- **construct_polynomial_feats**\n",
    "- **predict**\n",
    "- **linear_fit_closed**: You should use <samp>np.linalg.pinv</samp> in this function\n",
    "- **linear_fit_GD** (bonus for undergrad, **required for grad**):\n",
    "  $$ L_{\\text{linear, GD}}(\\theta) = \\dfrac{1}{2N} \\sum_{i=0}^{N} [y_i - \\hat{y}_i(\\theta)]^2 \\quad\\quad y_i = \\text{label}, \\, \\hat{y}_i(\\theta) = \\text{prediction} $$\n",
    "\n",
    "- **linear_fit_SGD** (bonus for undergrad, **required for grad**):\n",
    "  $$ L_{\\text{linear, SGD}}(\\theta) = \\dfrac{1}{2} [y_i - \\hat{y}_i(\\theta)]^2 \\quad\\quad y_i = \\text{label}, \\, \\hat{y}_i(\\theta) = \\text{prediction} $$\n",
    "\n",
    "- **ridge_fit_closed**: You should adjust your <samp>I</samp> matrix to handle the bias term differently than the rest of the terms\n",
    "- **ridge_fit_GD** (bonus for undergrad, **required for grad**):\n",
    "  $$ L_{\\text{ridge, GD}}(\\theta) = L_{\\text{linear, GD}}(\\theta) + \\dfrac{c_{\\lambda}}{2N}\\theta^T\\theta $$\n",
    "\n",
    "- **ridge_fit_SGD** (bonus for undergrad, **required for grad**):\n",
    "\n",
    "$$ L_{\\text{ridge, SGD}}(\\theta) = L_{\\text{linear, SGD}}(\\theta) + \\dfrac{c_{\\lambda}}{2N}\\theta^T\\theta $$\n",
    "\n",
    "- **ridge_cross_validation**: Use `ridge_fit_closed` for this function\n",
    "\n",
    "**IMPORTANT NOTE:**\n",
    "\n",
    "- Use your RMSE function to calculate actual loss when coding GD and SGD, but use the loss listed above to derive the gradient update.\n",
    "- In **ridge_fit_GD** and **ridge_fit_SGD**, you should avoid applying regularization to the bias term in the gradient update.\n",
    "\n",
    "The points for each function is in the **Deliverables and Points Distribution** section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Local Tests for Helper Regression Functions [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **regression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T00:32:38.739280500Z",
     "start_time": "2023-10-18T00:32:36.562645600Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.042699Z",
     "iopub.status.busy": "2024-10-05T00:17:01.042344Z",
     "iopub.status.idle": "2024-10-05T00:17:01.077999Z",
     "shell.execute_reply": "2024-10-05T00:17:01.076945Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestRegression\n",
    "\n",
    "unittest_reg = TestRegression()\n",
    "unittest_reg.test_rmse()\n",
    "unittest_reg.test_construct_polynomial_feats()\n",
    "unittest_reg.test_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Local Tests for Linear Regression Functions [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **regression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T00:32:42.694888800Z",
     "start_time": "2023-10-18T00:32:42.555083100Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.081604Z",
     "iopub.status.busy": "2024-10-05T00:17:01.081163Z",
     "iopub.status.idle": "2024-10-05T00:17:01.116670Z",
     "shell.execute_reply": "2024-10-05T00:17:01.115204Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestRegression\n",
    "\n",
    "unittest_reg = TestRegression()\n",
    "unittest_reg.test_linear_fit_closed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Local Tests for Ridge Regression Functions [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **regression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T00:32:45.526064400Z",
     "start_time": "2023-10-18T00:32:45.384302600Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.121284Z",
     "iopub.status.busy": "2024-10-05T00:17:01.120966Z",
     "iopub.status.idle": "2024-10-05T00:17:01.161500Z",
     "shell.execute_reply": "2024-10-05T00:17:01.160858Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestRegression\n",
    "\n",
    "unittest_reg = TestRegression()\n",
    "unittest_reg.test_ridge_fit_closed()\n",
    "unittest_reg.test_ridge_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Local Tests for Gradient Descent and SGD (Bonus for Undergrad Tests) [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **regression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T00:32:55.791534300Z",
     "start_time": "2023-10-18T00:32:55.698824300Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.165786Z",
     "iopub.status.busy": "2024-10-05T00:17:01.165212Z",
     "iopub.status.idle": "2024-10-05T00:17:01.210633Z",
     "shell.execute_reply": "2024-10-05T00:17:01.209533Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestRegression\n",
    "\n",
    "unittest_reg = TestRegression()\n",
    "unittest_reg.test_linear_fit_GD()\n",
    "unittest_reg.test_linear_fit_SGD()\n",
    "unittest_reg.test_ridge_fit_GD()\n",
    "unittest_reg.test_ridge_fit_SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5AioXetlpYO"
   },
   "source": [
    "### 3.3 Testing: General Functions and Linear Regression [5 pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In this section. we will test the performance of the linear regression. As long as your test RMSE score is close to the TA's answer (TA's answer $\\pm 0.05$), you can get full points.\n",
    "Let's first construct a dataset for polynomial regression.\n",
    "\n",
    "In this case, we construct the polynomial features up to degree 5.\n",
    "Each data sample consists of two features $[a,b]$. We compute the polynomial features of both $a$ and $b$ in order to yield the vectors $[1,a,a^2,a^3, \\ldots, a^{\\text{degree}}]$ and $[1,b,b^2,b^3, \\ldots, b^{\\text{degree}}]$. We train our model with the cartesian product of these polynomial features. The cartesian product generates a new feature vector consisting of all polynomial combinations of the features with degree less than or equal to the specified degree.\n",
    "\n",
    "For example, if degree = 2, we will have the polynomial features $[1,a,a^2]$ and $[1,b,b^2]$ for the datapoint $[a,b]$. The cartesian product of these two vectors will be $[1,a,b,ab,a^2,b^2]$. We do not generate $a^3$ and $b^3$ since their degree is greater than 2 (specified degree).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T00:33:01.017806100Z",
     "start_time": "2023-10-18T00:33:00.968259200Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.215197Z",
     "iopub.status.busy": "2024-10-05T00:17:01.214862Z",
     "iopub.status.idle": "2024-10-05T00:17:01.257963Z",
     "shell.execute_reply": "2024-10-05T00:17:01.256963Z"
    },
    "id": "A_DoBd-KtN7w"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from plotter import Plotter\n",
    "from regression import Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:32:24.406334600Z",
     "start_time": "2023-10-18T04:32:24.139368400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.261296Z",
     "iopub.status.busy": "2024-10-05T00:17:01.261039Z",
     "iopub.status.idle": "2024-10-05T00:17:01.323473Z",
     "shell.execute_reply": "2024-10-05T00:17:01.322022Z"
    },
    "id": "HmWNcjoplpYO",
    "outputId": "481f70d4-16f7-4d31-9aeb-11ddb364e822"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Generate a sample regression dataset with polynomial features\n",
    "# using the student's regression implementation.\n",
    "\n",
    "POLY_DEGREE = 5\n",
    "\n",
    "reg = Regression()\n",
    "plotter = Plotter(regularization=reg, poly_degree=POLY_DEGREE)\n",
    "\n",
    "x_all, y_all, p, x_all_feat = plotter.create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:32:59.928309800Z",
     "start_time": "2023-10-18T04:32:56.464708900Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.328855Z",
     "iopub.status.busy": "2024-10-05T00:17:01.327964Z",
     "iopub.status.idle": "2024-10-05T00:17:01.407411Z",
     "shell.execute_reply": "2024-10-05T00:17:01.406747Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Visualize simulated regression data\n",
    "\n",
    "plotter.plot_all_data(x_all, y_all, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAeNe7IRlpYO"
   },
   "source": [
    "In the figure above, the red curve is the true fuction we want to learn, while the blue dots are the noisy data points. The data points are generated by $Y=X\\theta+\\epsilon$, where $\\epsilon_i \\sim N(0,1)$ are i.i.d. generated noise.\n",
    "\n",
    "Now let's split the data into two parts, the training set and testing set. The yellow dots are for training, while the red dots are for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:33:02.812069800Z",
     "start_time": "2023-10-18T04:33:01.544488800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.429441Z",
     "iopub.status.busy": "2024-10-05T00:17:01.428973Z",
     "iopub.status.idle": "2024-10-05T00:17:01.484892Z",
     "shell.execute_reply": "2024-10-05T00:17:01.483885Z"
    },
    "id": "1-Ov5007lpYP",
    "outputId": "4b20d420-ff27-4f52-d74c-133c50a5ce01"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "xtrain, ytrain, xtest, ytest, train_indices, test_indices = plotter.split_data(\n",
    "    x_all, y_all\n",
    ")\n",
    "\n",
    "plotter.plot_split_data(xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train our model using the training set and see how our model performs on the testing set. Observe the red line, which is our model's learned function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:33:07.008266300Z",
     "start_time": "2023-10-18T04:33:05.804647600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.492334Z",
     "iopub.status.busy": "2024-10-05T00:17:01.491401Z",
     "iopub.status.idle": "2024-10-05T00:17:01.559396Z",
     "shell.execute_reply": "2024-10-05T00:17:01.557812Z"
    },
    "id": "VKBsoITElpYP",
    "outputId": "312c8796-4ae7-4ef9-a1bf-8444160fee3c"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Required for both Grad and Undergrad\n",
    "\n",
    "weight = reg.linear_fit_closed(x_all_feat[train_indices], y_all[train_indices])\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "print(\"Linear (closed) RMSE: %.4f\" % test_rmse)\n",
    "\n",
    "plotter.plot_linear_closed(xtrain, xtest, ytrain, ytest, x_all, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT:** If your RMSE is off, make sure to follow the instruction given for `linear_fit_closed` in the list of functions to implement above.\n",
    "\n",
    "Now let's use our linear gradient descent function with the same setup. Observe that the trendline is now less optimal, and our RMSE increased. Do not be alarmed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:33:24.562080500Z",
     "start_time": "2023-10-18T04:33:09.889136Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:01.571748Z",
     "iopub.status.busy": "2024-10-05T00:17:01.571146Z",
     "iopub.status.idle": "2024-10-05T00:17:12.714823Z",
     "shell.execute_reply": "2024-10-05T00:17:12.696664Z"
    },
    "id": "PXCxdWf3lpYP",
    "outputId": "db4e7947-cd88-4900-952d-57f449015cc8"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Required for Grad Only\n",
    "# This cell may take more than 1 minute\n",
    "weight, _ = reg.linear_fit_GD(\n",
    "    x_all_feat[train_indices], y_all[train_indices], epochs=50000, learning_rate=1e-8\n",
    ")\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print(\"Linear (GD) RMSE: %.4f\" % test_rmse)\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_pred = np.reshape(y_pred, (y_pred.size,))\n",
    "\n",
    "plotter.plot_linear_gd(xtrain, xtest, ytrain, ytest, x_all, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must tune our epochs and learning_rate. As we tune these parameters our trendline will approach the trendline generated by the linear closed form solution. Observe how we slowly tune (increase) the epochs and learning_rate below to create a better model.\n",
    "\n",
    "Note that the closed form solution will always give the most optimal/overfit results. We cannot outperform the closed form solution with GD. We can only approach closed forms level of optimality/overfitness. We leave the reasoning behind this as an exercise to the reader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:34:17.615900300Z",
     "start_time": "2023-10-18T04:33:27.621821700Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:12.754370Z",
     "iopub.status.busy": "2024-10-05T00:17:12.753723Z",
     "iopub.status.idle": "2024-10-05T00:17:32.715638Z",
     "shell.execute_reply": "2024-10-05T00:17:32.714860Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Required for Grad Only\n",
    "# This cell may take more than 1 minute\n",
    "\n",
    "learning_rates = [1e-8, 1e-6, 1e-4]\n",
    "weights = np.zeros((3, POLY_DEGREE**2 + 2))\n",
    "\n",
    "for ii in range(len(learning_rates)):\n",
    "    weights[ii, :] = reg.linear_fit_GD(\n",
    "        x_all_feat[train_indices],\n",
    "        y_all[train_indices],\n",
    "        epochs=50000,\n",
    "        learning_rate=learning_rates[ii],\n",
    "    )[0].ravel()\n",
    "    y_test_pred = reg.predict(\n",
    "        x_all_feat[test_indices], weights[ii, :].reshape((POLY_DEGREE**2 + 2, 1))\n",
    "    )\n",
    "    test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "    print(\"Linear (GD) RMSE: %.4f (learning_rate=%s)\" % (test_rmse, learning_rates[ii]))\n",
    "\n",
    "plotter.plot_linear_gd_tuninglr(\n",
    "    xtrain, xtest, ytrain, ytest, x_all, x_all_feat, learning_rates, weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR6MVOBKlpYP"
   },
   "source": [
    "And what if we just use the first 10 data points to train?\n",
    "\n",
    "### Linear Closed 10 Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:43:05.344451800Z",
     "start_time": "2023-10-18T04:43:04.916573Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:32.754016Z",
     "iopub.status.busy": "2024-10-05T00:17:32.752205Z",
     "iopub.status.idle": "2024-10-05T00:17:32.807910Z",
     "shell.execute_reply": "2024-10-05T00:17:32.806985Z"
    },
    "id": "pduyAUn8lpYP",
    "outputId": "710050e2-e45d-435c-fd47-d19da05259ef"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "rng = np.random.RandomState(seed=3)\n",
    "y_all_noisy = np.dot(x_all_feat, np.zeros((POLY_DEGREE**2 + 2, 1))) + rng.randn(\n",
    "    x_all_feat.shape[0], 1\n",
    ")\n",
    "sub_train = train_indices[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large RMSE values, rounding errors may result in larger than normal differences from the TA solution. Here, we will accept RMSE values $\\pm 0.1$ from the TA solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:43:07.728420500Z",
     "start_time": "2023-10-18T04:43:07.283493200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:32.812022Z",
     "iopub.status.busy": "2024-10-05T00:17:32.811738Z",
     "iopub.status.idle": "2024-10-05T00:17:32.939865Z",
     "shell.execute_reply": "2024-10-05T00:17:32.938840Z"
    },
    "id": "dh5wDvzNlpYQ",
    "outputId": "115c8c5b-4964-4ec4-d06b-a2d148285ab9"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Required for both Grad and Undergrad\n",
    "\n",
    "weight = reg.linear_fit_closed(x_all_feat[sub_train], y_all_noisy[sub_train])\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print(\"Linear (closed) 10 Samples RMSE: %.4f\" % test_rmse)\n",
    "\n",
    "plotter.plot_10_samples(\n",
    "    x_all, y_all_noisy, sub_train, y_pred, title=\"Linear Regression (Closed)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaS3FcNJlpYP"
   },
   "source": [
    "Did you see a worse performance? Let's take a closer look at what we have learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anXODlZmlpYQ"
   },
   "source": [
    "### 3.4 Testing: Ridge Regression [5 pts] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9cRpMWWlpYQ"
   },
   "source": [
    "Again, let's see what we have learned. **You only need to run the cell corresponding to your specific implementation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:45:14.993633Z",
     "start_time": "2023-10-18T04:45:14.750207800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:32.960444Z",
     "iopub.status.busy": "2024-10-05T00:17:32.959802Z",
     "iopub.status.idle": "2024-10-05T00:17:33.038923Z",
     "shell.execute_reply": "2024-10-05T00:17:33.036798Z"
    },
    "id": "Zze3sPKIlpYQ",
    "outputId": "20a228dd-08bf-4e10-8d5c-ae0b59224fac"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Required for both Grad and Undergrad\n",
    "weight = reg.ridge_fit_closed(\n",
    "    x_all_feat[sub_train], y_all_noisy[sub_train], c_lambda=10\n",
    ")\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print(\"Ridge Regression (closed) RMSE: %.4f\" % test_rmse)\n",
    "\n",
    "plotter.plot_10_samples(\n",
    "    x_all, y_all_noisy, sub_train, y_pred, title=\"Ridge Regression (Closed)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT:** Make sure to follow the instruction given for `ridge_fit_closed` in the list of functions to implement above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:54:32.661799400Z",
     "start_time": "2023-10-18T04:54:32.215747600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.046647Z",
     "iopub.status.busy": "2024-10-05T00:17:33.046071Z",
     "iopub.status.idle": "2024-10-05T00:17:33.236106Z",
     "shell.execute_reply": "2024-10-05T00:17:33.235285Z"
    },
    "id": "bY7DvrZPlpYQ",
    "outputId": "953cf781-398b-47c4-acec-2ffff6aa7665"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Required for Grad Only\n",
    "\n",
    "weight, _ = reg.ridge_fit_GD(\n",
    "    x_all_feat[sub_train], y_all_noisy[sub_train], c_lambda=20, learning_rate=1e-5\n",
    ")\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print(\"Ridge Regression (GD) RMSE: %.4f\" % test_rmse)\n",
    "\n",
    "plotter.plot_10_samples(\n",
    "    x_all, y_all_noisy, sub_train, y_pred, title=\"Ridge Regression (GD)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T04:56:13.048699400Z",
     "start_time": "2023-10-18T04:56:12.085819900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.245755Z",
     "iopub.status.busy": "2024-10-05T00:17:33.245411Z",
     "iopub.status.idle": "2024-10-05T00:17:33.351023Z",
     "shell.execute_reply": "2024-10-05T00:17:33.349933Z"
    },
    "id": "eOCxI-a5lpYQ",
    "outputId": "1ffc9f73-b1d0-4796-a353-25011ac89c0b"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Required for Grad Only\n",
    "\n",
    "weight, _ = reg.ridge_fit_SGD(\n",
    "    x_all_feat[sub_train], y_all_noisy[sub_train], c_lambda=20, learning_rate=1e-5\n",
    ")\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print(\"Ridge Regression (SGD) RMSE: %.4f\" % test_rmse)\n",
    "\n",
    "plotter.plot_10_samples(\n",
    "    x_all, y_all_noisy, sub_train, y_pred, title=\"Ridge Regression (SGD)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Linear vs. Ridge Regression Analysis [4pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Analyze the difference in performance between the linear and ridge regression methods given the output RMSE **from the testing on 10 samples** and their corresponding approximation plots.\n",
    "\n",
    "1. Why does ridge regression achieve a lower RMSE than linear regression on 10 sample points? **(1pts)** \n",
    "\n",
    "2. Describe and contrast two scenarios (real life applications): One where linear is more suitable than ridge, and one in which ridge is better choice than linear. Explain why. **(1 pts)**\n",
    "\n",
    "3. What is the impact of having some highly correlated features on the data set in terms of linear algebra? Mathematically explain (include expressions) how ridge has an advantage on this in comparison to linear regression. Include the idea of numerical stability. **(2pts)**\n",
    "\n",
    "Hint: Think about the closed form solution for the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Answer** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Answer** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Answer** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbfhyxI_lpYQ"
   },
   "source": [
    "### 3.6 Cross Validation Hyperparameter Search [5 pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "Let's use Cross Validation to search for the best value for `c_lambda` in ridge regression.\n",
    "\n",
    "Imagine we have a dataset of 10 points [1,2,3,4,5,6,7,8,9,10] and we want to do 5-fold cross validation.\n",
    "\n",
    "- The first iteration we would train with [3,4,5,6,7,8,9,10] and test (validate) with [1,2]\n",
    "- The second iteration we would train with [1,2,5,6,7,8,9,10] and test (validate) with [3,4]\n",
    "- The third iteration we would train with [1,2,3,4,7,8,9,10] and test (validate) with [5,6]\n",
    "- The fourth iteration we would train with [1,2,3,4,5,6,9,10] and test (validate) with [7,8]\n",
    "- The fifth iteration we would train with [1,2,3,4,5,6,7,8] and test (validate) with [9,10]\n",
    "\n",
    "\n",
    "We provided a list of possible values for $\\lambda$, and you will complete the `ridge_cross_validation` method to perform 5-fold cross-validation on the training data (we already use `train_indices` to get training data in the cell below). Split the training data into 5 folds, where 20 percent of the data will be used to test and 80 percent will be used to train. For each $\\lambda$, you will have calculated 5 RMSE values. We provide a function `hyperparameter_search` that takes the average of the RMSE values for each $\\lambda$ and picks the $\\lambda$ with the lowest mean RMSE. (Please look at hints for more information),\n",
    "\n",
    "**HINTS:**\n",
    "\n",
    "- `np.concatenate` is your friend\n",
    "- Make sure to follow the instruction given for `ridge_fit_closed` in the list of functions to implement above.\n",
    "- To use the 5-fold method, loop over all the data 5 times, where we split a different 20% of the data at every iteration. The first iteration extracts the first 20% for testing and the remaining 80% for training. The second iteration splits the second 20% of data for testing and the (different) remaining 80% for testing. If we have the array of elements 1 - 10, the second iteration would extract the numbers \"3\" and \"4\" because that's in the second 20% of the array.\n",
    "- The `hyperparameter_search` function will handle averaging the errors, so don't average the errors in `ridge_cross_validation`. We've done this so you can see your error across every fold when using the gradescope tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.364660Z",
     "iopub.status.busy": "2024-10-05T00:17:33.364220Z",
     "iopub.status.idle": "2024-10-05T00:17:33.421436Z",
     "shell.execute_reply": "2024-10-05T00:17:33.420246Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "lambda_list = [0.0001, 0.001, 0.1, 1, 5, 10, 50, 100, 1000, 10000]\n",
    "kfold = 5\n",
    "\n",
    "best_lambda, best_error, error_list = reg.hyperparameter_search(\n",
    "    x_all_feat[train_indices], y_all[train_indices], lambda_list, kfold\n",
    ")\n",
    "for lm, err in zip(lambda_list, error_list):\n",
    "    print(\"Lambda: %.4f\" % lm, \"RMSE: %.6f\" % err)\n",
    "\n",
    "print(\"Best Lambda: %.4f\" % best_lambda)\n",
    "weight = reg.ridge_fit_closed(\n",
    "    x_all_feat[train_indices], y_all_noisy[train_indices], c_lambda=best_lambda\n",
    ")\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print(\"Best Test RMSE: %.4f\" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4z1pZeSlpYR"
   },
   "source": [
    "### 3.7 Noisy Input Samples in Linear Regression [2.3% Bonus for All] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_i9qEyuMEbf"
   },
   "source": [
    "Consider a linear model of the form:\n",
    "\n",
    "$$\n",
    "y(x_n,\\theta) = \\theta_0 + \\sum_{d=1}^D\\theta_dx_{nd}\n",
    "$$\n",
    "\n",
    "where $x_n = (x_{n1}, \\ldots, x_{nD}) \\in \\mathbb{R}^{D}$ and weights $\\theta = (\\theta_0, \\ldots, \\theta_D) \\in \\mathbb{R}^{D}$. Given the the D-dimension input sample set $x = \\{ x_1, \\ldots, x_n\\}$ with corresponding target value $y = \\{y_1, \\ldots, y_n\\}$, the sum-of-squares error function is:\n",
    "\n",
    "$$\n",
    "E_D(\\theta) = \\frac{1}{2}\\sum_{n=1}^N\\left[y(x_n,\\theta)-y_n\\right]^2\n",
    "$$\n",
    "\n",
    "Now, suppose that Gaussian noise $\\epsilon_n \\in \\mathbb{R}^{D}$ is added independently to each of the input sample $x_n$ to generate a new sample set $x'= \\{x_1+\\epsilon_1, \\ldots, x_n+\\epsilon_n\\}$. Here, $\\epsilon_{ni}$ (an entry of $\\epsilon_n$) has zero mean and variance $\\sigma^2$. For each sample $x_n$, let $x_n' = (x_{n1} + \\epsilon_{n1}, \\ldots, x_{nD} + \\epsilon_{nd})$, where $n$ and $d$ is independent across both $n$ and $d$ indices.\n",
    "\n",
    "1. (0.7% Bonus) Show that $y(x_n',\\theta) = y(x_n, \\theta) + \\sum^D_{d=1}\\theta_d\\epsilon_{nd}$\n",
    "\n",
    "2. (1.6% Bonus) Assume the sum-of-squares error function of the noise sample set $x'= \\{x_1+\\epsilon_1, \\ldots, x_n+\\epsilon_n\\}$ is $E_D(\\theta)'$. Prove the expectation of $E_D(\\theta)'$ is equivalent to the sum-of-squares error $E_D(\\theta)$ for noise-free input samples with the addition of a weight-decay regularization term (e.g. $\\ell_2$ norm) , in which the bias parameter $\\theta_0$ is omitted from the regularizer. In other words, show that\n",
    "   $$\n",
    "   E[E_D(\\theta)'] = E_D(\\theta) + \\text{Regularizer}.\n",
    "   $$\n",
    "\n",
    "N.B. You should be incorporating your solution from the first part of this problem into the given sum of squares equation for the second part.\n",
    "\n",
    "Write your responses below using LaTeX in Markdown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYZ4uDIqMIFM",
    "tags": []
   },
   "source": [
    "**HINT:**\n",
    "\n",
    "- During the class, we have discussed how to solve for the weight $\\theta$ for ridge regression, the function looks like this:\n",
    "  $$E(\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\left[ y(x_i,\\theta)-y_i \\right]^2+\\frac{\\lambda}{N}\\sum_{i=1}^d |\\theta_i|^2$$\n",
    "  where the first term is the sum-of-squares error and the second term is the regularization term. N is the number of samples. In this question, we use another form of the ridge regression, which is:\n",
    "  $$\n",
    "  E(\\theta)=\\frac{1}{2}\\sum_{i=1}^N\\left[y(x_i,\\theta)-y_i \\right]^2+\\frac{\\lambda}{2}\\sum_{i=1}^d |\\theta_i|^2\n",
    "  $$\n",
    "- For the Gaussian noise $\\epsilon_n$, we have $E[\\epsilon_n]=0$\n",
    "\n",
    "- Assume the noise $\\epsilon = (\\epsilon_1,..., \\epsilon_n)$ are **independent** to each other, we have\n",
    "  $$\n",
    "  E[\\epsilon_n\\epsilon_m]=\\left\\{\n",
    "  \\begin{array}{rcl}\n",
    "  \\sigma^2       &      & m = n\\\\\n",
    "  0     &      & m \\neq n\\\\\n",
    "  \\end{array} \\right.\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Q4: Naive Bayes and Logistic Regression [39pts] <span style=\"color:blue\">**[P]**</span> | <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian classification, we're interested in finding the probability of a label given some observed feature vector $x = [x_{1}, \\ldots, x_{d}]$, which we can write as $P(y~|~{ x_{1}, \\ldots, x_{d}})$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(y~|~{ x_{1}, \\ldots, x_{d}}) = \\frac{P({ x_{1}, \\ldots, x_{d}}~|~y)P(y)}{P({ x_{1}, \\ldots, x_{d}})}\n",
    "$$\n",
    "\n",
    "The main assumption in Naive Bayes is that, given the label, the observed features are conditionally independent i.e.\n",
    "\n",
    "$$\n",
    "P({ x_{1}, \\ldots, x_{d}}~|~y) = P({x_{1}}~|~y) \\times \\ldots \\times P({x_{d}}~|~y)\n",
    "$$\n",
    "\n",
    "Now, let's apply this to a real-life scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Profile Screening [7pts] <span style=\"color:green\">[W]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Profile Screening using Naive Bayes [5pts]<span style=\"color:green\">[W]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rapidly evolving job market of Techlanta, a leading tech company has devised an automated resume screening system to assist in the hiring process for three distinct roles: Machine Learning Engineer, Data Analyst, and Product Manager. This system is designed to evaluate applicants based on five key binary attributes extracted from their resumes: {coding proficiency (1 for high, 0 for low), data analysis skills (1 for strong, 0 for weak), leadership experience (1 for yes, 0 for no), product design experience (1 for yes, 0 for no), marketing skills (1 for strong, 0 for weak)}.\n",
    "\n",
    "Aiming to ensure that the screening process is both fair and effective, the company is mindful of avoiding biases that could disadvantage any applicant. They have compiled a dataset of 12 anonymized resumes, with each position having 4 applicants, alongside feedback from previous interviews to serve as the ground truth.\n",
    "\n",
    "**Machine Learning Engineer** applicants have demonstrated attributes such as: {1, 1, 0, 0, 1}, {1, 1, 1, 0, 0}, {1, 0, 1, 1, 0}, {0, 1, 0, 1, 0}\n",
    "\n",
    "**Data Analyst** applicants are characterized by: {0, 1, 1, 0, 0}, {1, 0, 0, 0, 1}, {0, 1, 1, 0, 1}, {0, 1, 0, 1, 0}\n",
    "\n",
    "**Product Manager** applicants display: {0, 1, 1, 1, 0}, {0, 0, 1, 0, 1}, {1, 0, 1, 1, 1}, {0, 1, 0, 1, 1}.\n",
    "\n",
    "A new applicant's resume has been screened, and identified to **have** leadership, data analysis skills **with** product design experience. However, **does not have** coding experience or marketing skills.\n",
    "\n",
    "Now is the time to test your method!\n",
    "\n",
    "Using a multiclass Naive Bayes classifier, determine the most suitable job position for the new applicant.\n",
    "\n",
    "**NOTE: We expect students to show their work (prior probabilities, conditional probabilites, posterior probabilites) and not just the final answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 AI-Driven Profile Screening [2pts] <span style=\"color:green\">[W]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, human HR personnel review resumes to identify candidates who meet the minimum qualifications for a job. This process is subjective and can be influenced by conscious or unconscious biases. More recently, many organizations use Applicant Tracking Systems (ATS) to manage and screen resumes. ATS systems parse resumes to extract information like education, experience, skills, and other relevant details. They rank candidates based on how well their resume matches the job description and criteria set by the employer.\n",
    "\n",
    "More advanced systems incorporate AI to not only parse and match resumes but also to predict a candidate's job performance, cultural fit, and even retention likelihood. These systems can use machine learning models trained on historical hiring data, and they often employ a combination of keyword matching, machine learning models (such as decision trees, support vector machines, and neural networks), and natural language processing (NLP) techniques. \n",
    "\n",
    "In recent years, several high-profile cases have highlighted the challenges of bias in AI-driven resume screening processes. For example:\n",
    "\n",
    "- In 2018, Amazon had to abandon its AI recruitment tool because it was trained on a decade's worth of resumes, predominantly from men, leading to a bias against women's resumes, such as penalizing resumes that mentioned \"women's\" clubs or activities.<sup>[1](#footnote1)</sup>\n",
    "- UnitedHealth Group faced allegations of racial bias in their AI-driven hiring tool. The system reportedly favored white applicants over black applicants. The company discontinued the tool after these concerns were raised.<sup>[2](#footnote1)</sup>.\n",
    "- HireVue’s AI tool analyzes video interviews, and has been used by more than a 100 companies on over a million applicants, according to the Washington Post<sup>[3](#footnote1)</sup>.  Notice that the algorithm can learn historical patterns in the data (e.g., gender, race, socioeconomic status) and be more likely to mark “traditional” applicants (white, male, able-bodied) as more employable.<sup>[4](#footnote1)</sup> As a result, applicants who deviate from the “traditional”—including people don’t speak English as a native language or who are disabled—are likely to get lower scores.<sup>[5](#footnote1)</sup>\n",
    "- LinkedIn’s job-matching AI was found to exhibit gender biases in job recommendations, a consequence of the training data's inherent patterns. The algorithms ranked candidates based on their likelihood to apply for a position or respond to a recruiter. As a result, more men were referred for open roles due to their proactive approach in seeking new opportunities.<sup>[6](#footnote1)</sup>\n",
    "\n",
    "*Sources:*\n",
    "\n",
    "<a name=\"footnote1\">1</a>: [Guardian](https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine).\n",
    "<a name=\"footnote1\">2</a>: [The Wall street Journal](https://www.wsj.com/articles/new-york-regulator-probes-unitedhealth-algorithm-for-racial-bias-11572087601).\n",
    "<a name=\"footnote1\">3</a>: [Washington Post](https://www.washingtonpost.com/technology/2019/11/06/prominent-rights-group-files-federal-complaint-against-ai-hiring-firm-hirevue-citing-unfair-deceptive-practices/).\n",
    "<a name=\"footnote1\">4</a>: [MIT Technology Review](https://www.technologyreview.com/2019/11/07/75194/hirevue-ai-automated-hiring-discrimination-ftc-epic-bias/).\n",
    "<a name=\"footnote1\">5</a>: [Brookings](https://www.brookings.edu/articles/for-some-employment-algorithms-disability-discrimination-by-default/).\n",
    "<a name=\"footnote1\">6</a>: [MIT Technology Review](https://www.technologyreview.com/2021/06/23/1026825/linkedin-ai-bias-ziprecruiter-monster-artificial-intelligence/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the context above, which of the following approaches is most effective for mitigating bias in AI-driven resume screening systems? (Choose all that apply.)\n",
    "\n",
    "A. Increasing the size of the training dataset\n",
    "\n",
    "B. Conducting regular audits of the AI system's decisions\n",
    "\n",
    "C. Utilizing differential privacy techniques (introducing \"noise\" or subtle alterations to the data in a way that protects individual privacy while allowing the aggregate patterns to remain intact)\n",
    "\n",
    "D. Relying solely on keyword matching to ensure objectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2 **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 News Data Sentiment Classification via Logistic Regression [30pts] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains the sentiments for financial news headlines from the perspective of a retail investor. The sentiment of news has 3 classes, negative, positive and neutral. In this problem, we only use the negative (class label = 0) and positive (class label = 1) classes for binary logistic regression. For data preprocessing, we remove the duplicate headlines and remove the neutral class to get 1967 unique news headlines. Then we randomly split the 1967 headlines into training set and evaluation set with 8:2 ratio. We use the training set to fit a binary logistic regression model.\n",
    "\n",
    "The code which is provided loads the documents, preprocess the data, builds a [“bag of words” representation](https://en.wikipedia.org/wiki/Bag-of-words_model) of each document. Your task is to complete the missing portions of the code in <strong>logisticRegression.py</strong> to determine whether a news headline is negative or positive.\n",
    "\n",
    "In <strong>logistic_regression.py</strong> file, complete the following functions:\n",
    "\n",
    "- <strong>sigmoid</strong>: transform $s = x\\theta$ to probability of being positive using sigmoid function, which is $\\frac{1}{1+e^{-s}}$.\n",
    "- **bias_augment**: augment $x$ with 1's to account for bias term in $\\theta$\n",
    "- **predict_probs**: predicts the probability of positive label $P(y = 1 | x)$\n",
    "- **predict_labels**: predicts labels\n",
    "- <strong>loss</strong>: calculates binary cross-entropy loss\n",
    "- <strong>gradient</strong>: calculate the gradient of the loss function with respect to the parameters $\\theta$.\n",
    "- <strong>accuracy</strong>: calculate the accuracy of predictions\n",
    "- **evaluate**: gives loss and accuracy for a given set of points\n",
    "- <strong>fit</strong>: fit the logistic regression model on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression Overview:\n",
    "\n",
    "1. In logistic regression, we model the conditional probability using parameters $\\theta$, which includes a bias term b.\n",
    "   $$p(y_i=1\\, |\\, x_i;\\theta )\\, =\\, {h}_{\\theta }(x_i) = {\\sigma}(x\\theta)$$\n",
    "   $$p(y_i=0\\, |\\, x_i;\\theta )\\, =\\, {1-h}_{\\theta }(x_i) $$\n",
    "\n",
    "where $\\sigma(\\cdot)$ is the sigmoid function as follows:\n",
    "$$\\sigma(s) = \\frac{1}{1+e^{-s}}$$\n",
    "\n",
    "2. The conditional probabilities of the positive class $(y=1)$ and the negative class $(y=0)$ of the sample $x_i$ attributes are combined into one equation as follows:\n",
    "\n",
    "$$ p(y*i\\, |\\, x_i;\\theta )\\, =\\, {({h}*{\\theta }(x*i))}^{y_i}\\, {(1-{h}*{\\theta }(x_i))}^{1-y_i} $$\n",
    "\n",
    "3. Assuming that the samples are independent of each other, the likelihood of the entire dataset is the product of the probabilities of all samples. We use maximum likehood estimation to estimate the model parameters $\\theta$. The negative log likelihood (scaled by the dataset size $N$) is given by:\n",
    "   $$ \\mathcal{L}(\\theta \\mid X, Y)\\, \\, =-\\frac{1}{N}\\, \\sum ^{N}_{i=1} {{y}\\_i}\\log{h_\\theta(x*i)}+\\, {(1-{y}\\_i)}\\log(1-h*\\theta({x}\\_i)) $$\n",
    "\n",
    "where:\n",
    "\n",
    "$N =$ number of training samples  \n",
    "$x_i =$ <i>bag of words</i> features of the i-th training sample  \n",
    "$y_i =$ label of the i-th training sample\n",
    "\n",
    "Note that this will be our model's loss function\n",
    "\n",
    "4. Then calculate the gradient $\\triangledown_\\theta\\mathcal{L}$ and use gradient descent to optimize the loss function:\n",
    "   $$\\theta_{t+1} = \\theta_{t} - \\eta \\cdot \\triangledown_\\theta\\mathcal{L}(\\theta_t \\mid X, Y)$$\n",
    "\n",
    "where $\\eta$ is the learning rate and the gradient $\\triangledown_\\theta\\mathcal{L}$ is given by:\n",
    "\n",
    "$$\n",
    "\\triangledown_\\theta \\mathcal{L}(\\theta \\mid X, Y) = \\frac{1}{N} \\sum_{i=1}^{N} x_{i}^{\\top} \\left( h_{\\theta}(x_i) - y_i \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Local Tests for Logistic Regression [No Points]\n",
    "\n",
    "You may test your implementation of the functions contained in **logistic_regression.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.432342Z",
     "iopub.status.busy": "2024-10-05T00:17:33.431863Z",
     "iopub.status.idle": "2024-10-05T00:17:33.483563Z",
     "shell.execute_reply": "2024-10-05T00:17:33.482657Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestLogisticRegression\n",
    "\n",
    "unittest_lr = TestLogisticRegression()\n",
    "unittest_lr.test_sigmoid()\n",
    "unittest_lr.test_bias_augment()\n",
    "unittest_lr.test_loss()\n",
    "unittest_lr.test_predict_probs()\n",
    "unittest_lr.test_predict_labels()\n",
    "unittest_lr.test_loss()\n",
    "unittest_lr.test_accuracy()\n",
    "unittest_lr.test_evaluate()\n",
    "unittest_lr.test_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Logistic Regression Model Training [No Points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.488097Z",
     "iopub.status.busy": "2024-10-05T00:17:33.487587Z",
     "iopub.status.idle": "2024-10-05T00:17:33.532499Z",
     "shell.execute_reply": "2024-10-05T00:17:33.531329Z"
    },
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from logistic_regression import LogisticRegression as LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.541438Z",
     "iopub.status.busy": "2024-10-05T00:17:33.540277Z",
     "iopub.status.idle": "2024-10-05T00:17:33.975556Z",
     "shell.execute_reply": "2024-10-05T00:17:33.973169Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "news_data = pd.read_csv(\"./data/news-data.csv\", encoding=\"cp437\", header=None)\n",
    "\n",
    "class_to_label_mappings = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "label_to_class_mappings = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "news_data.columns = [\"Sentiment\", \"News\"]\n",
    "news_data.drop_duplicates(inplace=True)\n",
    "\n",
    "news_data = news_data[news_data.Sentiment != \"neutral\"]\n",
    "\n",
    "news_data[\"Sentiment\"] = news_data[\"Sentiment\"].map(class_to_label_mappings)\n",
    "\n",
    "vectorizer = text.CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "X = news_data[\"News\"].values\n",
    "y = news_data[\"Sentiment\"].values.reshape(-1, 1)\n",
    "\n",
    "RANDOM_SEED = 5\n",
    "BOW = vectorizer.fit_transform(X).toarray()\n",
    "indices = np.arange(len(news_data))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
    "    BOW, y, indices, test_size=0.2, random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the training data\n",
    "Try different learning rates `lr` and number of `epochs` to achieve >80% test accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:17:33.982379Z",
     "iopub.status.busy": "2024-10-05T00:17:33.981659Z",
     "iopub.status.idle": "2024-10-05T00:18:48.803948Z",
     "shell.execute_reply": "2024-10-05T00:18:48.799585Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "model = LogReg()\n",
    "lr = 0.05\n",
    "epochs = 10000\n",
    "threshold = 0.5\n",
    "theta = model.fit(X_train, y_train, X_test, y_test, lr, epochs, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Logistic Regression Model Evaluation [No Points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:48.850878Z",
     "iopub.status.busy": "2024-10-05T00:18:48.847806Z",
     "iopub.status.idle": "2024-10-05T00:18:49.062138Z",
     "shell.execute_reply": "2024-10-05T00:18:49.060402Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, theta, threshold)\n",
    "print(f\"Test Dataset Accuracy: {round(test_acc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss function on the training data and the test data for every 100th epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:49.076414Z",
     "iopub.status.busy": "2024-10-05T00:18:49.070630Z",
     "iopub.status.idle": "2024-10-05T00:18:49.550979Z",
     "shell.execute_reply": "2024-10-05T00:18:49.547943Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the accuracy function on the training data and the test data for each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:49.563649Z",
     "iopub.status.busy": "2024-10-05T00:18:49.562805Z",
     "iopub.status.idle": "2024-10-05T00:18:49.722895Z",
     "shell.execute_reply": "2024-10-05T00:18:49.720989Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "model.plot_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out sample evaluations from the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:49.730047Z",
     "iopub.status.busy": "2024-10-05T00:18:49.728988Z",
     "iopub.status.idle": "2024-10-05T00:18:49.797811Z",
     "shell.execute_reply": "2024-10-05T00:18:49.795434Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "num_samples = 10\n",
    "for i in range(10):\n",
    "    rand_index = np.random.randint(0, len(X_test))\n",
    "    x_test = np.reshape(X_test[rand_index], (1, X_test.shape[1]))\n",
    "    prob = model.predict_probs(model.bias_augment(x_test), theta)\n",
    "    pred = model.predict_labels(prob, threshold)\n",
    "    print(f\"Input News: {X[indices_test[rand_index]]}\\n\")\n",
    "    print(f\"Predicted Sentiment: {label_to_class_mappings[pred[0][0]]}\")\n",
    "    print(f\"Actual Sentiment: {label_to_class_mappings[y_test[rand_index][0]]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Logistic Regression Model Threshold Experiments [2pts] <span style=\"color:green\">[W]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the sigmoid function in a logistic regression model outputs a decimal value between 0 and 1. For a classification problem, we need a threshold to determine which outputs are considered as positive and which are considered as negatives.\n",
    "\n",
    "Instead of the default threshold of 0.5, let's train 4 different logistic regression models each with a different threshold as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "thresholds = [0.1, 0.3, 0.7, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# This cell may take about 5 minutes to run\n",
    "\n",
    "thresholds.sort()\n",
    "model = LogReg()\n",
    "lr = 0.05\n",
    "epochs = 10000\n",
    "outputs = []\n",
    "for t in thresholds:\n",
    "    theta = model.fit(X_train, y_train, X_test, y_test, lr, epochs, t)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, theta, t)\n",
    "    outputs.append(\n",
    "        f\"Test Dataset Accuracy: {round(test_acc, 3)} with threshold of {round(t, 2)}\"\n",
    "    )\n",
    "for o in outputs:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the final accuracies of the model with different thresholds in the cell output from above. Which threshold would you pick for the model and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Feature Selection Implementation [30 pts] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is an integral aspect of machine learning. It is the process of selecting a subset of relevant features that are to be used as the input for the machine learning task. Feature selection may lead to simpler models for easier interpretation, shorter training times, avoidance of the curse of dimensionality, and better generalization by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Reduction [30pts] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the <strong>feature_reduction.py</strong> file, complete the following functions:\n",
    "\n",
    "- <strong>forward_selection</strong>\n",
    "- <strong>backward_elimination</strong>\n",
    "\n",
    "**Reminder:** A p-value is known as the observed significance value for a null hypothesis. In our case, the p-value of a feature is associated with the hypothesis $H_{0}\\colon \\beta_j = 0$. If $\\beta_j = 0$, then this feature contributes no predictive power to our model and should be dropped. We reject the null hypothesis if the p-value is smaller than our significance level. In short, a p-value is a measure of how much the given feature significantly represents an observed change. **A lower p-value represents higher significance.** Some more information about p-values can be found here: https://www.youtube.com/watch?v=vemZtEM63GY\n",
    "\n",
    "#### Forward Selection:\n",
    "\n",
    "In forward selection, we start with a null model and fit the model with one individual feature at a time. We then select the most significant feature with the lowest p-value. We continue to do this until we try to select a feature with a p-value >= significance level. This implies that all remaining features are insignificant with p-values < significance level and that no more features should be added.\n",
    "\n",
    "Steps to implement it:\n",
    "\n",
    "1. Choose a significance level (provided to you)\n",
    "1. Start with an empty list of selected features\n",
    "2. For each feature NOT yet included in the selected features:\n",
    "    - Fit a simple regression model using the the selected features AND the feature under consideration\n",
    "    - Record the p-value of the feature under consideration \n",
    "3. Find the feature with the minimum p-value.\n",
    "    - If the feature's p-value < significance level, ADD the feature to the selected features and repeat from Step 2.\n",
    "    - Otherwise, stop and return the selected features\n",
    "\n",
    "#### Backward Elimination:\n",
    "\n",
    "In backward elimination, we start with a full model and then remove the most insignificant feature with the highest p-value. We continue to do this untilwe try to remove a feature with p-value < significance level. This implies that all of the remaining features are significant with pvalues < significance level, and should therefore be kept.\n",
    "\n",
    "Steps to implement it:\n",
    "\n",
    "1. Choose a significance level (provided to you)\n",
    "2. Start with a full list of ALL features as selected features.\n",
    "3. Fit a simple regression model using the selected features\n",
    "4. Find the feature with the maximum p-value.\n",
    "    - If the feature's p-value >= significance level, REMOVE the feature from the selected features and repeat from Step 3.\n",
    "    - Otherwise, stop and return the selected features. \n",
    "\n",
    "**HINT:** Use `sm.OLS` as your regression model (documentation [here](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html)). Be sure to add bias to your regression model by augmenting your data using the `sm.add_constants` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestFeatureReduction\n",
    "\n",
    "unittest_feature_reduction = TestFeatureReduction()\n",
    "unittest_feature_reduction.test_forward_selection()\n",
    "unittest_feature_reduction.test_backward_elimination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from feature_reduction import FeatureReduction\n",
    "\n",
    "bc_dataset = load_breast_cancer()\n",
    "bc = pd.DataFrame(bc_dataset.data, columns=bc_dataset.feature_names)\n",
    "print(\"Dataset Features: \", bc.columns.tolist())\n",
    "bc[\"Diagnosis\"] = bc_dataset.target\n",
    "\n",
    "X = bc.drop(\"Diagnosis\", axis=1)\n",
    "y = bc[\"Diagnosis\"]\n",
    "featureselection = FeatureReduction()\n",
    "# Run the functions to make sure two feature lists are generated, one for each method\n",
    "forward_selection_feature_list = FeatureReduction.forward_selection(X, y, 0.1)\n",
    "backward_selection_feature_list = FeatureReduction.backward_elimination(X, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# View selected features and perform linear regression using the selected features\n",
    "\n",
    "print(\"Forward Feature Selection\")\n",
    "FeatureReduction.evaluate_features(X, y, forward_selection_feature_list)\n",
    "print(\"Backward Feature Elimination\")\n",
    "FeatureReduction.evaluate_features(X, y, backward_selection_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Imbalanced Classes in Classification Tasks [5.6% Bonus For All] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many datasets, the representation of classes in the training data is unequal. For example, most transactions in a banking system are legitimate, most images of manufactured parts show no visual defect, most email attachments are entirely benign. However, when you have highly imbalanced data, your model may converge to a highly biased estimator, classifying most inputs to the majority class. This is a valid solution, after all, the model is simplying converging based on the a priori distribution of classes in the training data, but we still want our model to have accurate prediction on the minority class.\n",
    "\n",
    "To illustrate this point, take a look at the following 2D artificial dataset with a high class imbalance, and the results of training a classifier on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "SEED = 1\n",
    "imb = 0.99\n",
    "\n",
    "# Generate an artifical 10D classification problem with a high degree of class imbalance.\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    weights=[imb],\n",
    "    n_features=10,\n",
    "    n_informative=10,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0,\n",
    "    class_sep=0.8,\n",
    "    random_state=SEED,\n",
    ")\n",
    "# Splitting the data into training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Plot both the training and testing data.\n",
    "a, b = 0, 5\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].scatter(\n",
    "    X_train[:, a],\n",
    "    X_train[:, b],\n",
    "    c=y_train,\n",
    "    cmap=matplotlib.colors.ListedColormap([\"red\", \"blue\"]),\n",
    "    s=1,\n",
    ")\n",
    "axs[0].title.set_text(f\"Training Data\")\n",
    "axs[1].scatter(\n",
    "    X_test[:, a],\n",
    "    X_test[:, b],\n",
    "    c=y_test,\n",
    "    cmap=matplotlib.colors.ListedColormap([\"red\", \"blue\"]),\n",
    "    s=1,\n",
    ")\n",
    "axs[1].title.set_text(f\"Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is higher than 2 dimensional, but we can visualize any dim-2 subspace, just for understanding purposes. If you want, you can tinker with `a` and `b` to choose different subspaces, though most won't be particularly informative. Note that while the data doesn't look even vaguely separable in 2D, distances stack up as you add dimensions, so in 10D, this data is separable.\n",
    "\n",
    "Let's run a basic classifier on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:49.807375Z",
     "iopub.status.busy": "2024-10-05T00:18:49.806722Z",
     "iopub.status.idle": "2024-10-05T00:18:49.894226Z",
     "shell.execute_reply": "2024-10-05T00:18:49.892442Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "classifier = SVC(random_state=SEED)  # sklearn's Support Vector Classifier\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predicted = classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy of this prediction.\n",
    "print(f\"Accuracy: {100*accuracy_score(y_test, y_predicted)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier achieved very good accuracy, but as we'll see, accuracy is not always a perfect measure for determining the goodness of a model, since it can hide a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 A More Comprehensive Measure [1.75% Bonus] <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better view of our model's accuracy, we need to generate a confusion matrix. Each data point in the test set has a true class value and a predicted class value. A confusion matrix counts the instances of every possible combination of test and prediction values.\n",
    "\n",
    "In the **smote.py** file, complete the following:\n",
    "\n",
    "1. **generate_confusion_matrix**: Given the true test labels and the predicted labels from a model, generate a confusion matrix. C[i, j] should denote the number of instances where a sample from class i was predicted to be in class j. Even though our example is binary classification, your code should work for an arbitrary number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:49.903256Z",
     "iopub.status.busy": "2024-10-05T00:18:49.902365Z",
     "iopub.status.idle": "2024-10-05T00:18:50.307607Z",
     "shell.execute_reply": "2024-10-05T00:18:50.303656Z"
    }
   },
   "outputs": [],
   "source": [
    "from smote import SMOTE\n",
    "from utilities.localtests import TestSMOTE\n",
    "\n",
    "unittest_sm = TestSMOTE()\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "unittest_sm.test_simple_confusion_matrix()\n",
    "unittest_sm.test_complex_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix of the prediction we made.\n",
    "from smote import confusion_matrix_vis\n",
    "\n",
    "confusion_matrix_vis(sm.generate_confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from a first look, this relatively high accuracy is clearly not a good measure of the model's performance, as the model is highly biased. This (albeit naive) model is very accurate when given a point from class 0. However, when given a point from class 1, it's much less inaccurate.\n",
    "\n",
    "Depending on the application, e.g., cancer screening or facial recognition, you may want to prioritize minimizing false negatives or false positives depending on your application's objectives, in which case you might have to accept a low degree of accuracy for some classes, but in this exploration, we just want a balanced performance across our labels. We need a measure of test performance that not only conveys the accuracy of the model, but is robust against bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score (or F-Measure) is one metric that seeks to measure the performance of a model across all classes. F1 score can be calculated on any class, and here, we'll calculate it every class.\n",
    "\n",
    "In the **smote.py** file, complete the following:\n",
    "\n",
    "2. **f1_scores**: Given the confusion matrix from a classification, calculate the F1 scores for each class. To calculate the F1 score, take the harmonic mean of the precision and recall for that class's predictions. Even though our example is binary classification, your code should work for an arbitrary number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:50.318250Z",
     "iopub.status.busy": "2024-10-05T00:18:50.317734Z",
     "iopub.status.idle": "2024-10-05T00:18:50.634661Z",
     "shell.execute_reply": "2024-10-05T00:18:50.631540Z"
    }
   },
   "outputs": [],
   "source": [
    "unittest_sm.test_simple_f1s()\n",
    "unittest_sm.test_complex_f1s()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our new metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:50.650929Z",
     "iopub.status.busy": "2024-10-05T00:18:50.649288Z",
     "iopub.status.idle": "2024-10-05T00:18:51.136004Z",
     "shell.execute_reply": "2024-10-05T00:18:51.134361Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = sm.generate_confusion_matrix(y_test, y_predicted)\n",
    "print(f\"Accuracy: {100*accuracy_score(y_test, y_predicted)}%\")\n",
    "print(f\"F1 Scores: {np.round(sm.f1_scores(conf), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:49.903256Z",
     "iopub.status.busy": "2024-10-05T00:18:49.902365Z",
     "iopub.status.idle": "2024-10-05T00:18:50.307607Z",
     "shell.execute_reply": "2024-10-05T00:18:50.303656Z"
    }
   },
   "source": [
    "F1 takes the range $[0, 1]$, with 1 meaning perfect precision and recall, so we obviously have some room for improvement now, since the F1 score for the majority class is significantly higher than that of the minority class.\n",
    "\n",
    "Note that the work you did corresponded to 1 value per class. In literature, you may see some variations on F1 score. First of all, the 1 in F1 is actually a parameter. There are an infinite number of $F_\\beta$ scores, though $F_1$ is most common. Second, you may see a single F1 score on binary tasks where the performance on only 1 class is important for the downstream system. And finally, you may see these values averaged across the classes. Macro-F1 refers to the arithmetic mean of all F1 scores. Weighted-F1 refers to the arithmetic mean weighted by class size.\n",
    "\n",
    "For our example, we'll keep both F1 values so we can see the relative change in performance between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance bias, many models employ a technique called class weighting. Essentially, when the model fit or iterative training is performed, the loss/gain/effect (varies from model to model) caused by each point in the training set can be weighted by the inverse of the proportion of that point's class. Applied to what you just implemented (logistic regression), that might look like this:\n",
    "$$\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_{t} - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{N} x_{i}^{\\top} \\left( h_{\\theta}(x_i) - y_i \\right)\\\\\n",
    "&\\downarrow\\\\\n",
    "\\theta_{t+1} &= \\theta_{t} - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{N} x_{i}^{\\top} \\left( h_{\\theta}(x_i) - y_i \\right) \\cdot\\mathbf{ \\frac{N}{\\vert\\{y\\in Y | y=y_i\\}\\vert}}\\hspace{0.5cm}\\left(\\frac{\\textit{total \\# of points}}{\\textit{\\# of points in }y_i\\textit{'s class}}\\right)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class weighting is generally applicable, and is often the preferred solution, but on some models it's inaccessible, unwieldy, or produces undesirable results. There is an alternative solution: instead of changing the model, eliminate the problem. If our training data had balanced classes, we could just run the model. This is the idea behind oversampling. Oversampling refers to the practice of populating the input space with points from the input space itself.\n",
    "\n",
    "Though, if you just sampled minority classes with replacement, you would get duplicates of the same point in the training data. This can genuinely help (since it approximates the class weight technique), but there is a more sophisticated solution: the Synthetic Minority Oversampling TEchnique (SMOTE). You can read the original paper [on the arxiv](https://arxiv.org/abs/1106.1813) for your own edification, but we'll be implementing a slightly different version, so the details in this Notebook and the function docstrings are sufficient to complete this HW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 SMOTE [3.85% Bonus] <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly sampling the points from the minority class to bring it up to size, SMOTE samples a training point from the minority, samples another training point from the minority class that is \"within a neighborhood around the first point,\" then randomly linearly interpolates between them to generate a new \"synthetic\" point lying on the line segment drawn between those two points. In this section, we're going to focus in on our binary classification problem, so we will only be oversampling the points from the one minority class. Additionally, our algorithm will only oversample to equality (the point at which the two classes are of equal size). Though, in practice, the amount you oversample becomes a hyperparameter to your model, which you can sweep with cross-validation.\n",
    "\n",
    "In the **smote.py** file, complete the following:\n",
    "\n",
    "1. **interpolate**: Given a start point, an end point, and an interpolation coefficient, return a linearly interpolated point.\n",
    "\n",
    "2. **k_nearest_neighbors**: Given some set of points (N, D) and a parameter k, generate an (N, k) array of indices such that `output[i]` contains the k indices corresponding to the k nearest neighbors of point i.\n",
    "\n",
    "3. **smote**: Given some data `X` (|maj|+|min|, D) and their binary labels `y` (|maj|+|min|,), generate |maj|-|min| new synthetic points from the minority class and return only those new synthetic points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:51.141874Z",
     "iopub.status.busy": "2024-10-05T00:18:51.141435Z",
     "iopub.status.idle": "2024-10-05T00:18:51.504966Z",
     "shell.execute_reply": "2024-10-05T00:18:51.503776Z"
    }
   },
   "outputs": [],
   "source": [
    "unittest_sm.test_interpolate()\n",
    "unittest_sm.test_knn()\n",
    "unittest_sm.test_smote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply SMOTE to our dataset and see the downstream effect on our classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:51.510507Z",
     "iopub.status.busy": "2024-10-05T00:18:51.509552Z",
     "iopub.status.idle": "2024-10-05T00:18:52.227619Z",
     "shell.execute_reply": "2024-10-05T00:18:52.221940Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Run SMOTE!\n",
    "X_train_synth, y_train_synth = sm.smote(X_train, y_train, k=5, inter_coeff_range=(0, 1))\n",
    "# Combine synthetic data with original data.\n",
    "X_train_balanced = np.vstack((X_train, X_train_synth))\n",
    "y_train_balanced = np.hstack((y_train, y_train_synth))\n",
    "\n",
    "# Visualize\n",
    "a, b = 0, 5\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].scatter(\n",
    "    X_train[:, a],\n",
    "    X_train[:, b],\n",
    "    c=y_train,\n",
    "    cmap=matplotlib.colors.ListedColormap([\"red\", \"blue\"]),\n",
    "    s=1,\n",
    ")\n",
    "axs[0].title.set_text(\"Training Data - Unbalanced\")\n",
    "axs[1].scatter(\n",
    "    X_train_balanced[:, a],\n",
    "    X_train_balanced[:, b],\n",
    "    c=y_train_balanced,\n",
    "    cmap=matplotlib.colors.ListedColormap([\"red\", \"blue\"]),\n",
    "    s=1,\n",
    ")\n",
    "axs[1].title.set_text(\"Training Data - Balanced by SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Original\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predicted1 = classifier.predict(X_test)\n",
    "\n",
    "conf1 = sm.generate_confusion_matrix(y_test, y_predicted1)\n",
    "print(\"Original performance:\")\n",
    "confusion_matrix_vis(conf1)\n",
    "print(f\"Accuracy: {100*accuracy_score(y_test, y_predicted1)}%\")\n",
    "print(f\"F1 Scores: {sm.f1_scores(conf1)}\")\n",
    "\n",
    "# Balanced\n",
    "classifier.fit(X_train_balanced, y_train_balanced)\n",
    "y_predicted2 = classifier.predict(X_test)\n",
    "\n",
    "print(\"\\n\\n\\nPerformance after SMOTE:\")\n",
    "conf2 = sm.generate_confusion_matrix(y_test, y_predicted2)\n",
    "confusion_matrix_vis(conf2)\n",
    "print(f\"SMOTEd Accuracy: {100*accuracy_score(y_test, y_predicted2)}%\")\n",
    "print(f\"SMOTEd F1 Scores: {sm.f1_scores(conf2)}\")\n",
    "\n",
    "# If you're getting errors running this cell, check your implementation, then regenerate the SMOTE data by rerunning the previous cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new synthetic data (if done correctly), the model is now significantly more performant on the test points for the minority class! The F1 scores aren't equal, but you could probably continue upping the number of synthetic minority points created until that equality is reached. Again, the amount of points we choose to generate with SMOTE is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, SMOTE has limitations.\n",
    "\n",
    "- Since SMOTE only works by interpolating on a line segment, SMOTE generates points within the convex hull of the input data. For classes with non-convex or multi-modal spatial distributions, SMOTE can actually ruin the quality of the class's representation in the input space for certain choices of k.\n",
    "- For other types of data, like images, linear interpolation in general will not produce any meaningful data. You may have to use some semantic or generative combination instead.\n",
    "- Additionally, SMOTE is very susceptible to outliers. Without undergoing some filtering, SMOTE will generate unreasonable points when interpolating with an outlier.\n",
    "- Often, improving performance on the minority class can decrease performance on the majority class. Thus, if you want to reduce bias, sometimes you have to accept lower accuracy as well.\n",
    "\n",
    "In general, just consider this another tool in your toolbox. It won't work on everything, but it will work exceptionally well on a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class imbalance problem shows up all across ML and when creating human-facing ML models, the class imbalance problem has an additional ethical element tied on; since a lot of data collection is heavily biased to people of higher socio-economic status in a society, the resulting models trained from it may have higher performance quality for high SES individuals. Moreover, this isn't the only kind of data collection bias that currently exists. In general, a model that is performant to only a particular class of people will create an inherently unjust system that has the potential to relatively worsen the lives of those who are less represented in the training data, especially since some users will use your model as though it is foolproof.\n",
    "\n",
    "This class imbalance problem will also probably show up in your project (if you're doing classification)! Remember that accuracy isn't everything, and analysis of your system's bias is also important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: Netflix Movie Recommendation Problem Solved using SVD [2.1% Bonus for All] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to tackle the famous problem of movie recommendation using just our SVD functions that we have implemented.\n",
    "We are given a table of reviews that 600+ users have provided for close to 10,000 different movies. Our challenge is to predict how much a user would rate a movie that they have not seen (or rated) yet.\n",
    "Once we have these ratings, we would then be able to predict which movies to recommend to that user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding How SVD Helps in Movie Recommendation\n",
    "\n",
    "We are given a dataset of user-movie ratings ($R$) that looks like the following:\n",
    "\n",
    "<img src=\"./data/svd_recommender_matrix.png\" height=\"200\" width=\"800\" />\n",
    "\n",
    "Ratings in the matrix range from 1-5. In addition, the matrix contains `nan` wherever there is no rating provided by the user for the corresponding movie. One simple way to utilize this matrix to predict movie ratings for a given user-movie pair would be to fill in each row / column with the average rating for that row / column.\n",
    "For example: For each movie, if any rating is missing, we could just fill in the average value of all available ratings and expect this to be around the actual / expected rating.\n",
    "\n",
    "While this may sound like a good approximation, it turns out that by just using SVD we can improve the accuracy of the predicted rating.\n",
    "\n",
    "##### How does SVD fit into this picture?\n",
    "\n",
    "Recall how we previously used SVD to compress images by throwing out less important information. We could apply the same idea to our above matrix ($R$) to generate another matrix ($R\\_$) which will provide the same information, i.e ratings for any user-movie pairs but by combining only the most important features.\n",
    "\n",
    "Let's look at this with an example:\n",
    "\n",
    "Assume that decomposition of matrix $R$ looks like:\n",
    "\n",
    "$$\n",
    "R = U\\Sigma V^{T}\n",
    "$$\n",
    "\n",
    "We can re-write this decomposition as follows:\n",
    "\n",
    "$$\n",
    "R = U\\sqrt\\Sigma \\sqrt\\Sigma V^{T}\n",
    "$$\n",
    "\n",
    "If we were to take only the top K singular values from this matrix, we could again write this as:\n",
    "\n",
    "$$\n",
    "R\\_ = U\\sqrt\\Sigma_k \\sqrt\\Sigma_k V^{T}\n",
    "$$\n",
    "\n",
    "Thus we have now effectively separated our ratings matrix $R$ into two matrices given by:\n",
    "$\n",
    "U_k = U_{[:k]}\\sqrt\\Sigma_k\n",
    "$\n",
    "and\n",
    "$\n",
    "V_k = \\sqrt\\Sigma_k V_{[:k]}^{T}\n",
    "$\n",
    "\n",
    "There are many ways to visualize the importance of $U$ and $V$ matrices but with respect to our context of movie ratings, we can visualize these matrices as follows:\n",
    "\n",
    "<img src=\"./data/svd_recommender_decomposition_matrix.png\" height=\"400\" width=\"700\" />\n",
    "\n",
    "We can imagine each row of $U_k$ to be holding some information how much each user likes a particular feature (feature1, feature2, feature 3...feature $k$). On the contrary, we can imagine each column of $V_k^{T}$ to be holding some information about how much each movie relates to the given features (feature 1, feature 2, feature 3 ... feature $k$).\n",
    "\n",
    "Lets denote the row of $U_k$ by $u_i$ and the column of $V_k^{T}$ by $m_j$. Then the dot-product: $u_i \\cdot m_j$ can provide us with information on how much a user _i_ likes movie _j_.\n",
    "\n",
    "##### What have we achieved by doing this?\n",
    "\n",
    "Starting with a matrix $R$ containing very few ratings, we have been able to summarize the sparse matrix of ratings into matrices $U_k$ and $V_k$ which each contain feature vectors about the Users and the Movies. Since these feature vectors are summarized from only the most important K features (by our SVD), we can predict any User-Movie rating that is closer to the actual value than just taking any average rating of a row / column (recall our brute force solution discussed above).\n",
    "\n",
    "Now this method in practice is still not close to the state-of-the-art but for a naive and simple method we have used, we can still build some powerful visualizations as we will see in part 3.\n",
    "\n",
    "We have divided the task into 3 parts:\n",
    "\n",
    "1. Implement `recommender_svd` to return matrices $U_k$ and $V_k$\n",
    "\n",
    "2. Implement `predict` to predict top 3 movies a given user would watch\n",
    "\n",
    "3. _(Ungraded)_ Feel free to run the final cell labeled to see some visualizations of the feature vectors you have generated\n",
    "\n",
    "Hint: Movie IDs are IDs assigned to the movies in the dataset and can be greater than the number of movies. This is why we have given movies_index and users_index as well that map between the movie IDs and the indices in the ratings matrix. Please make sure to use this as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:59.943986Z",
     "iopub.status.busy": "2024-10-05T00:18:59.942252Z",
     "iopub.status.idle": "2024-10-05T00:18:59.984837Z",
     "shell.execute_reply": "2024-10-05T00:18:59.983933Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from regression import Regression\n",
    "from svd_recommender import SVDRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:18:59.989548Z",
     "iopub.status.busy": "2024-10-05T00:18:59.988858Z",
     "iopub.status.idle": "2024-10-05T00:19:02.667270Z",
     "shell.execute_reply": "2024-10-05T00:19:02.665927Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "recommender = SVDRecommender()\n",
    "recommender.load_movie_data()\n",
    "regression = Regression()\n",
    "# Read the data into the respective train and test dataframes\n",
    "train, test = recommender.load_ratings_datasets()\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Train Dataset Stats:\")\n",
    "print(\"Shape of train dataset: {}\".format(train.shape))\n",
    "print(\"Number of unique users (train): {}\".format(train[\"userId\"].unique().shape[0]))\n",
    "print(\"Number of unique users (train): {}\".format(train[\"movieId\"].unique().shape[0]))\n",
    "print(\"Sample of Train Dataset:\")\n",
    "print(\"------------------------------------------\")\n",
    "print(train.head())\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Test Dataset Stats:\")\n",
    "print(\"Shape of test dataset: {}\".format(test.shape))\n",
    "print(\"Number of unique users (test): {}\".format(test[\"userId\"].unique().shape[0]))\n",
    "print(\"Number of unique users (test): {}\".format(test[\"movieId\"].unique().shape[0]))\n",
    "print(\"Sample of Test Dataset:\")\n",
    "print(\"------------------------------------------\")\n",
    "print(test.head())\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# We will first convert our dataframe into a matrix of Ratings: R\n",
    "# R[i][j] will indicate rating for movie:(j) provided by user:(i)\n",
    "# users_index, movies_index will store the mapping between array indices and actual userId / movieId\n",
    "R, users_index, movies_index = recommender.create_ratings_matrix(train)\n",
    "print(\"Shape of Ratings Matrix (R): {}\".format(R.shape))\n",
    "\n",
    "# Replacing `nan` with average rating given for the movie by all users\n",
    "# Additionally, zero-centering the array to perform SVD\n",
    "mask = np.isnan(R)\n",
    "masked_array = np.ma.masked_array(R, mask)\n",
    "r_means = np.array(np.mean(masked_array, axis=0))\n",
    "R_filled = masked_array.filled(r_means)\n",
    "R_filled = R_filled - r_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 SVD Recommender [2.1% Bonus for All] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In <strong>svd_recommender.py</strong> file, complete the following function:\n",
    "\n",
    "- <strong>recommender_svd</strong>: Use the above equations to output $U_k$ and $V_k$. You can utilize the `svd` and `compress` methods from `imgcompression.py` to retrieve your initial $U$, $\\Sigma$ and $V$ matrices. Then, calculate $U_k$ and $V_k$ based on the decomposition example above.\n",
    "\n",
    "- <strong>predict</strong>: Predict the next 3 movies (sorted by high to low rating) that the user would be most interested in watching among the ones above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to predict movies that a user would be interested in watching next. Since our dataset contains a large list of movies and our model is very naive, filtering among this huge set for top 3 movies can produce results that we may not correlate immediately. Therefore, we'll restrict this prediction to only movies among a subset as given by movies_pool.\n",
    "\n",
    "Let us consider a user (ID: 660) who has already watched and rated well (>3) on the following movies: <br>\n",
    "\n",
    "- Iron Man (2008)\n",
    "- Thor: The Dark World (2013)\n",
    "- Avengers, The (2012)\n",
    "\n",
    "The following cell tries to predict which among the movies given by the list below, the user would be most interested in watching next: <br>\n",
    "`movies_pool`:\n",
    "\n",
    "- Ant-Man (2015)\n",
    "- Iron Man 2 (2010)\n",
    "- Avengers: Age of Ultron (2015)\n",
    "- Thor (2011)\n",
    "- Captain America: The First Avenger (2011)\n",
    "- Man of Steel (2013)\n",
    "- Star Wars: Episode IV - A New Hope (1977)\n",
    "- Ladybird Ladybird (1994)\n",
    "- Man of the House (1995)\n",
    "- Jungle Book, The (1994)\n",
    "\n",
    "**HINT: You can use the method `get_movie_id_by_name` to convert movie names into movie IDs and vice-versa.**\n",
    "\n",
    "**NOTE: The user may have already watched and rated some of the movies in `movies_pool`. Remember to filter these out before returning the output. The original Ratings Matrix, $R$ might come in handy here along with `np.isnan`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Local Test for `recommender_svd` Function [No Points]\n",
    "\n",
    "You may test your implementation of the function in the cell below. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:19:02.672529Z",
     "iopub.status.busy": "2024-10-05T00:19:02.672193Z",
     "iopub.status.idle": "2024-10-05T00:19:02.874563Z",
     "shell.execute_reply": "2024-10-05T00:19:02.872655Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestSVDRecommender\n",
    "\n",
    "unittest_svd_rec = TestSVDRecommender()\n",
    "unittest_svd_rec.test_recommender_svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:19:02.882352Z",
     "iopub.status.busy": "2024-10-05T00:19:02.882017Z",
     "iopub.status.idle": "2024-10-05T00:21:27.973399Z",
     "shell.execute_reply": "2024-10-05T00:21:27.966770Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Implement the method `recommender_svd` and run it for the following values of features\n",
    "no_of_features = [2, 3, 8, 15, 18, 25, 30]\n",
    "test_errors = []\n",
    "\n",
    "for k in no_of_features:\n",
    "    U_k, V_k = recommender.recommender_svd(R_filled, k)\n",
    "    pred = []  # to store the predicted ratings\n",
    "    for _, row in test.iterrows():\n",
    "        user = row[\"userId\"]\n",
    "        movie = row[\"movieId\"]\n",
    "        u_index = users_index[user]\n",
    "        # If we have a prediction for this movie, use that\n",
    "        if movie in movies_index:\n",
    "            m_index = movies_index[movie]\n",
    "            pred_rating = np.dot(U_k[u_index, :], V_k[:, m_index]) + r_means[m_index]\n",
    "        # Else, use an average of the users ratings\n",
    "        else:\n",
    "            pred_rating = np.mean(np.dot(U_k[u_index], V_k)) + r_means[m_index]\n",
    "        pred.append(pred_rating)\n",
    "    test_error = regression.rmse(test[\"rating\"], pred)\n",
    "    test_errors.append(test_error)\n",
    "    print(\"RMSE for k = {} --> {}\".format(k, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Test Error over the different values of `k`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:21:27.995390Z",
     "iopub.status.busy": "2024-10-05T00:21:27.994218Z",
     "iopub.status.idle": "2024-10-05T00:21:28.549213Z",
     "shell.execute_reply": "2024-10-05T00:21:28.548228Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "fig = plt.figure()\n",
    "plt.plot(no_of_features, test_errors, \"bo\")\n",
    "plt.plot(no_of_features, test_errors)\n",
    "plt.xlabel(\"Value for k\")\n",
    "plt.ylabel(\"RMSE on Test Dataset\")\n",
    "plt.title(\"SVD Recommendation Test Error with Different k values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Local Test for `predict` Functions [No Points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may test your implementation of the function in the cell below. See [Using the Local Tests](#using_local_tests) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:21:28.556492Z",
     "iopub.status.busy": "2024-10-05T00:21:28.555902Z",
     "iopub.status.idle": "2024-10-05T00:21:47.123361Z",
     "shell.execute_reply": "2024-10-05T00:21:47.121658Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "unittest_svd_rec.test_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Visualize Movie Vectors [No Points]\n",
    "\n",
    "Our model is still a very naive model, but it can still be used for some powerful analysis such as clustering similar movies together based on user's ratings.\n",
    "\n",
    "We have said that our matrix $V_k$ that we have generated above contains information about movies. That is, each column in $V_k$ contains (feature 1, feature 2, .... feature $k$) for each movie. We can also say this in other terms that $V_k$ gives us a feature vector (of length k) for each movie that we can visualize in a $k$-dimensional space. For example, using this feature vector, we can find out which movies are similar or vary.\n",
    "\n",
    "While we would love to visualize a $k$-dimensional space, the constraints of our 2D screen wouldn't really allow us to do so. Instead let us set $K=2$ and try to plot the feature vectors for just a couple of these movies.\n",
    "\n",
    "As a fun activity run the following cell to visualize how our model separates the two sets of movies given below.\n",
    "\n",
    "**NOTE:** There are 2 possible visualizations. Your plot could be the one that's given on the expected PDF or the one where the y-coordinates are inverted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T00:21:47.129122Z",
     "iopub.status.busy": "2024-10-05T00:21:47.128669Z",
     "iopub.status.idle": "2024-10-05T00:22:01.885579Z",
     "shell.execute_reply": "2024-10-05T00:22:01.884451Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "marvel_movies = [\n",
    "    \"Thor: The Dark World (2013)\",\n",
    "    \"Avengers: Age of Ultron (2015)\",\n",
    "    \"Ant-Man (2015)\",\n",
    "    \"Iron Man 2 (2010)\",\n",
    "    \"Avengers, The (2012)\",\n",
    "    \"Thor (2011)\",\n",
    "    \"Captain America: The First Avenger (2011)\",\n",
    "]\n",
    "marvel_labels = [\"Blue\"] * len(marvel_movies)\n",
    "star_wars_movies = [\n",
    "    \"Star Wars: Episode IV - A New Hope (1977)\",\n",
    "    \"Star Wars: Episode V - The Empire Strikes Back (1980)\",\n",
    "    \"Star Wars: Episode VI - Return of the Jedi (1983)\",\n",
    "    \"Star Wars: Episode I - The Phantom Menace (1999)\",\n",
    "    \"Star Wars: Episode II - Attack of the Clones (2002)\",\n",
    "    \"Star Wars: Episode III - Revenge of the Sith (2005)\",\n",
    "]\n",
    "star_wars_labels = [\"Green\"] * len(star_wars_movies)\n",
    "\n",
    "\n",
    "movie_titles = star_wars_movies + marvel_movies\n",
    "genre_labels = star_wars_labels + marvel_labels\n",
    "\n",
    "movie_indices = [\n",
    "    movies_index[recommender.get_movie_id_by_name(str(x))] for x in movie_titles\n",
    "]\n",
    "\n",
    "_, V_k = recommender.recommender_svd(R_filled, k=2)\n",
    "x, y = V_k[0, movie_indices], V_k[1, movie_indices]\n",
    "fig = plt.figure()\n",
    "plt.scatter(x, y, c=genre_labels)\n",
    "for i, movie_name in enumerate(movie_titles):\n",
    "    plt.annotate(movie_name, (x[i], y[i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
