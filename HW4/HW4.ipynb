{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjp__3bRh42K"
   },
   "source": [
    "## Fall 2024 CS 4641/7641 A: Machine Learning Homework 4\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Monday, Dec 2nd, 2024 11:59 pm EST\n",
    "\n",
    "### <font color='cyan'>For Homework 4, the December 2nd deadline is a hard and strict deadline. This means that this deadline cannot be even extended for students with GT-approved accomodations.</font>\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "\n",
    "- Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "  <font color='darkred'>\n",
    "- Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own.</font>\n",
    "  <font color='darkred'>\n",
    "- All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the instituteâ€™s Academic Integrity procedures. If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, **WE WILL DIRECTLY REPORT ALL CASES TO OSI**, which may, unfortunately, lead to a very harsh outcome. **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSJJRXYL2yOY"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- Unless a theory question explicitly states that no work is required to be shown, you must provide an explanation, justification, or calculation for your answer.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "- You can directly type Latex equations into markdown cells.\n",
    "- If a question requires a picture, you could use this syntax `<img src=\"\" style=\"width: 300px;\"/>` to include them within your ipython notebook.\n",
    "\n",
    "- Your write up must be submitted in PDF format. Please ensure all questions are answered within the Jupyter Notebook using either Markdown or LaTeX. <font color = 'darkred'>We will **NOT** accept handwritten work. </font> Make sure that your work is formatted correctly, for example submit $\\sum_{i=0} x_i$ instead of \\text{sum\\_\\{i=0\\} x_i}\n",
    "- When submitting the non-programming part of your assignment, you must correctly map pages of your PDF to each question/subquestion to reflect where they appear. <font color='darkred'>**Improperly mapped questions may not be graded correctly and/or will result in point deductions for the error.**</font>\n",
    "- All assignments should be done individually, and each student must write up and submit their own answers.\n",
    "- **Graduate Students**: You are required to complete any sections marked as Bonus for Undergrads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN-9x-Qm2yOY"
   },
   "source": [
    "## Using the autograder\n",
    "\n",
    "- You will find three assignments (for grads) on Gradescope that correspond to HW4: \"Assignment 4 Programming\", \"Assignment 4 - Non-programming\" and \"Assignment 4 Programming - Bonus for all\". Undergrads will have an additional assignment called \"Assignment 4 Programming - Bonus for Undergrads\".\n",
    "- You will submit your code for the autograder in the Assignment 4 Programming sections. Please refer to the Deliverables and Point Distribution section for what parts are considered required, bonus for undergrads, and bonus for all\".\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue\n",
    "- **For the \"Assignment 4 - Non-programming\" part, you will need to submit to Gradescope a PDF copy of your Jupyter Notebook with the cells ran. [See this EdStem Post for multiple ways on to convert your .ipynb into a .pdf file.](https://edstem.org/us/courses/58807/discussion/4999496)** Please refer to the **Deliverables and Point Distribution** section for an outline of the non-programming questions.\n",
    "- **When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem. The pages in the PDF should be of size 8.5\" x 11\", otherwise there may be a deduction in points for extra long sheets.**\n",
    "- You **MUST** pass the Autograder Test to gain points for the programming section. There will not be any partial credit or manual grading for this part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "609MA8lGdPzV"
   },
   "source": [
    "## Using the local tests <a id='using_local_tests'></a>\n",
    "\n",
    "- For some of the programming questions we have included a local test using a small toy dataset to aid in debugging. The local test sample data and outputs are stored in localtests.py\n",
    "- There are no points associated with passing or failing the local tests, you must still pass the autograder to get points.\n",
    "- **It is possible to fail the local test and pass the autograder** since the autograder has a certain allowed error tolerance while the local test allowed error may be smaller. Likewise, passing the local tests does not guarantee passing the autograder.\n",
    "- **You do not need to pass both local and autograder tests to get points, passing the Gradescope autograder is sufficient for credit.**\n",
    "- It might be helpful to comment out the tests for functions that have not been completed yet.\n",
    "- It is recommended to test the functions as it gets completed instead of completing the whole class and then testing. This may help in isolating errors. Do not solely rely on the local tests, continue to test on the autograder regularly as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N09bYK1-MnO1"
   },
   "source": [
    "## Deliverables and Points Distribution\n",
    "\n",
    "### Q1: Classification with Two Layer NN [80 pts: 55pts + 25pts Grad / 3.3% Undergrad Bonus]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>NN.py</font>\n",
    "\n",
    "- **1.1 NN Implementation** [65pts: 50pts + 15pts Grad / 2% **Bonus for Undergrad**] - _programming_\n",
    "\n",
    "  - SiLU [5pts]\n",
    "\n",
    "  - Softmax [5pts]\n",
    "\n",
    "  - Cross Entropy loss [5pts]\n",
    "\n",
    "  - dropout [5pts]\n",
    "\n",
    "  - forward propagation and with and without dropout [5pts + 5pts]\n",
    "\n",
    "  - compute gradients and update weights [2.5pts + 2.5pts]\n",
    "\n",
    "  - backward without momentum [5pt]\n",
    "\n",
    "  - Gradient Descent [5pts]\n",
    "\n",
    "  - Batch Gradient Descent [10pts Grad / 1.3% **Bonus for Undergrad**]\n",
    "\n",
    "  - Momentum [5pts Grad / 0.7% **Bonus for Undergrad**]\n",
    "\n",
    "- **1.2 Loss plot and CE for Gradient Descent** [5pts] - _programming_\n",
    "\n",
    "- **1.3 Loss plot and CE for Batch Gradient Descent** [5pts Grad / 0.7% **Bonus for Undergrad**] - _programming_\n",
    "\n",
    "- **1.4 Loss plot and CE value for NN with Gradient Descent with Momentum** [5pts Grad / 0.6% **Bonus for Undergrad**] - _programming_\n",
    "\n",
    "### Q2: CNN [20pts Grad / 2.7% Bonus for Undergrad + 1.1% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>cnn.py, cnn_image_transformations.py and Written Report</font>\n",
    "\n",
    "- **2.1 Image Classification using Pytorch CNN** [20pts Grad / 2.7% **Bonus for Undergrad**]\n",
    "\n",
    "  - 2.1.1 Loading the Model [5pts Grad / 0.7% **Bonus for Undergrad**] - _programming_\n",
    "\n",
    "  - 2.1.2 Building the Model [5pts Grad / 0.7% **Bonus for Undergrad**] - _non-programming_\n",
    "\n",
    "  - 2.1.3 Training the Model [8pts Grad / 1% **Bonus for Undergrad**] - _non-programming_\n",
    "\n",
    "  - 2.1.4 Examining Accuracy and Loss [2pts Grad / 0.3% **Bonus for Undergrad**] - _non-programming_\n",
    "\n",
    "- **2.2 Exploring Deep CNN Architectures** [1.1% **Bonus for All**] - _non-programming_\n",
    "\n",
    "### Q3: Random Forest [40pts + 2.1% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>random_forest.py and Written Report</font>\n",
    "\n",
    "- **3.1 Random Forest Implementation** [35pts] - _programming_\n",
    "\n",
    "- **3.2 Hyperparameter Tuning with a Random Forest** [5pts] - _programming_\n",
    "\n",
    "- **3.3 Plotting Feature Importance** [1.1% **Bonus for All**] - _non-programming_\n",
    "\n",
    "- **3.4 ADABoost [1% **Bonus for All**]** - _programming_\n",
    "\n",
    "### Q4: SVM [15 pts]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>Written Report</font>\n",
    "\n",
    "- **4.1 Fitting an SVM Classifier** [10 pts]\n",
    "\n",
    "  - 4.1.1 Fit the SVM Classifier [7 pts] - _non programming_\n",
    "\n",
    "  - 4.1.2 Plot the SVM Classifier [3 pts] - _non programming_\n",
    "\n",
    "- **4.2 Using Kernels** [5 pts] - _non programming_\n",
    "\n",
    "### Q5: Next Character Prediction using Recurrent Neural Networks (RNNs) [6.8% Bonus for all]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>rnn.py, lstm.py, base_sequential_model.py and Written Report</font>\n",
    "\n",
    "- **5.1: Model Architecture** [4.4% **Bonus for All**] - _programming_\n",
    "  - 5.1.1: Defining the Simple RNN model [2.2% **Bonus for All**]\n",
    "  - 5.1.2: Defining the LSTM model [2.2% **Bonus for All**]\n",
    "\n",
    "- **5.2: Simple RNN vs LSTM Model Text Generation Training Comparison Analysis** [2.4% **Bonus for All**] - _non programming_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points Totals:\n",
    "- Total Base: 150 pts for grads / 105 pts for undergrads\n",
    "- Total Undergrad Bonus: 6%\n",
    "- Total Bonus for All: 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "Submit the following files to their respective assignments on Gradescope for the programming portions:\\n\",\n",
    "\n",
    "- **Assignment 4 Programming**\n",
    "    - nn.py\n",
    "    - random_forest.py\n",
    "    - cnn_image_transformations.py (for grads only)\n",
    "\n",
    "- **Assignment 4 Programming - Bonus for All**\n",
    "    - random_forest.py\n",
    "    - lstm.py\n",
    "    - rnn.py\n",
    "    - base_sequential_model.py\n",
    "\n",
    "- **Assignment 4 Programming - Bonus for Undergrad**\n",
    "    - NN.py\n",
    "    - cnn_image_transformations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "## Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0Ui6T2as9iI",
    "outputId": "d9a440f2-4324-48ea-d869-5c575b4a565f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utilities.utils import get_housing_dataset\n",
    "\n",
    "print(\"Version information\")\n",
    "\n",
    "print(\"python: {}\".format(sys.version))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"numpy: {}\".format(np.__version__))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding and Emissions\n",
    "\n",
    "Coding and computational research contribute to greenhouse gas emissions. The main source of these emissions is the power draw of computers during compute- and data-intensive computational analyses. In 2020, the sector of information and communication technologies was responsible for between 1.8% and 2.8% of GHG emissions, surprisingly more than the sector of aviation <sup>[<a href=\"https://www.nature.com/articles/s43588-023-00506-2\">1</a>]</sup>. Machine learning models, especially large ones, can consume significant amounts of energy during training and inference, which contributes to greenhouse gas emissions. Artificial intelligence, including large language models, is also a significant emitter of carbon <sup>[<a href=\"https://www.nature.com/articles/s42256-022-00529-w\">2</a>]</sup>.\n",
    "\n",
    "Carbon footprint of coding impacts several Sustainable Development Goals (SDGs), particularly SDG 13 (Climate Action) and SDG 12 (Responsible Consumption and Production).<sup>[<a href=\"https://www.undp.org/sustainable-development-goals\">3</a>]</sup> This means writing clean and efficient code transcends functionalityâ€”itâ€™s an environmental imperative. As coders, we can play a role in mitigating this impact.\n",
    "\n",
    "### Measuring Our Impact:\n",
    "\n",
    "CodeCarbon estimates the amount of CO2 produced by the cloud or personal computing resources used to execute the code<sup>[<a href=\"https://codecarbon.io/\">4</a>]</sup> . \n",
    "\n",
    "Using CodeCarbon in your upcoming assignment will help you understand the environmental impact of your code and explore ways to reduce it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq4QZ4su2yOd"
   },
   "source": [
    "# 1: Two Layer Neural Network [80 pts; 55pts + 25pts Grad / 3.3% Undergrad Bonus] <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiDi0B5ZMnO9"
   },
   "source": [
    "## 1.1 NN Implementation [65pts; 50pts + 15pts Grad / 2% Bonus for Undergrad] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In this section, you will implement a two layer fully connected neural network to perform a Classification Task. You will also experiment with different activation functions and optimization techniques. We provide two activation functions here - SiLU and Softmax. You will implement a neural network where the first hidden layer uses a SiLU activation and the output layer uses Softmax.\n",
    "\n",
    "You'll also implement Gradient Descent (GD) and Batch Gradient Descent (BGD) algorithms for training these neural nets. **GD is mandatory for all. BGD is bonus for undergraduate students but mandatory for graduate students.**\n",
    "\n",
    "In the <strong>NN.py</strong> file, complete the following functions:\n",
    "\n",
    "- <strong>silu</strong>\n",
    "- <strong>derivative_silu</strong>\n",
    "- <strong>softmax</strong>\n",
    "- <strong>cross_entropy_loss</strong>\n",
    "- <strong>\\_dropout</strong>\n",
    "- <strong>forward</strong>\n",
    "- <strong>compute_gradients</strong>\n",
    "- <strong>update_weights</strong>\n",
    "- <strong>backward</strong>\n",
    "- <strong>gradient_descent</strong>\n",
    "- <strong>batch_gradient_descent</strong>:<span style=\"color:darkred\"> **Mandatory for graduate students, bonus for undergraduate students.**</span> Please batch your data in a wraparound manner. For example, given a dataset of 9 numbers, [1, 2, 3, 4, 5, 6, 7, 8, 9], and a batch size of 6, the first iteration batch will be [1, 2, 3, 4, 5, 6], the second iteration batch will be [7, 8, 9, 1, 2, 3], the third iteration batch will be [4, 5, 6, 7, 8, 9], etc...\n",
    "\n",
    "We'll train this neural network on sklearn's California Housing dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "There are many activation functions that are used for various purposes. For this question, we use SiLU and the softmax activation functions. We encourage you to explore the plethora of options, many of which are listed on [Wikipedia](https://en.wikipedia.org/wiki/Activation_function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "The sigmoid function is a non-linear function with an S-shaped curve and is regarded as a foundational activation function. Its output is in the range $(0, 1)$, making it the function to use for binary classification output. The function is expressed as $$o = \\phi(u)=\\frac{1}{1+e^{-u}}$$<br> The derivation of the sigmoid function is given by $$o' = \\phi'(u) = \\frac{1}{1+e^{-u}} \\left(1-\\frac{1}{1+e^{-u}}\\right) = o(1-o)$$\n",
    "\n",
    "<b>Note:</b> We do not use sigmoid in this homework; it is only included for the sake of completeness.\n",
    "\n",
    "![sigmoid](data/images/sigmoid.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax is a common activation function used in neural networks, especially for multiclass classification problems like the one we are tackling. It is used to convert a vector of raw outputs from the last layer of the Neural Network into a probability distribution over multiple classes. The softmax function takes as input a vector of real numbers and transforms them into a probability distribution, ensuring that the probabilities sum to 1.\n",
    "\n",
    "Mathematically, given an input vector of [x1, x2, ..., xn], the softmax function calculates the probability p(y=i) for each class i as follows:\n",
    "\n",
    "p(y=i) = $e^{xi} / (e^{x1} + e^{x2} + ... + e^{xn})$\n",
    "\n",
    "![sigmoid](data/images/softmax.png)\n",
    "\n",
    "As discussed in class, the equation that we will use in this Neural network accounts for both the x values and the weights:\n",
    "\n",
    "![sigmoid](data/images/softmaxNew.jpg)\n",
    "\n",
    "<strong>TODO:</strong> Implement the function <strong>softmax</strong> in <strong>NN.py</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_softmax\").test_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiLU (Sigmoid Linear Unit)\n",
    "\n",
    "The Sigmoid Linear Unit (SiLU), also known as the Swish activation function, is defined as:\n",
    "\n",
    "$$\n",
    "o = \\phi(u) = u \\cdot \\sigma(u)\n",
    "$$\n",
    "\n",
    "where $\\sigma(u)$ is the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(u) = \\frac{1}{1 + e^{-u}}\n",
    "$$\n",
    "\n",
    "![SiLU](data/images/silu.png)\n",
    "\n",
    "The derivative of SiLU, $\\phi'(u)$, is given by:\n",
    "\n",
    "$$\n",
    "\\phi'(u) = \\sigma(u) \\cdot (1 + u \\cdot (1 - \\sigma(u)))\n",
    "$$\n",
    "\n",
    "Unlike ReLU, SiLU is a smooth and non-linear activation function that retains gradients for negative inputs, which helps during training by improving gradient flow and enabling better convergence.\n",
    "\n",
    "In this homework, we implement SiLU.\n",
    "\n",
    "![Derivate SiLu](data/images/derivative_silu.png)\n",
    "\n",
    "<strong>TODO:</strong> Implement the function <strong>silu</strong> and <strong>derivative_silu</strong> in <strong>NN.py</strong>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_silu\").test_silu()\n",
    "TestNN(\"test_d_silu\").test_d_silu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "A single layer perceptron can be thought of as a linear hyperplane as in logistic regression followed by a non-linear activation function. $$u_{i} = \\sum \\limits_{j=1}^{d} \\theta_{ij}x_{j}+b_{i}$$ $$o_{i} = \\phi \\left( \\sum \\limits_{j=1}^{d} \\theta_{ij}x_{j}+b_{i} \\right) = \\phi(\\theta_{i}^{T}x+b_{i})$$ where $x$ is a d-dimensional vector i.e. $x \\in R^{d}$. It is one datapoint with $d$ features. $\\theta_{i} \\in R^{d}$ is the weight vector for the $i^{th}$ hidden unit, $b_{i} \\in R$ is the bias element for the $i^{th}$ hidden unit and $\\phi(.)$ is a non-linear activation function that has been described below. $u_{i}$ is a linear combination of the features in $x_j$ weighted by $\\theta_{i}$ whereas $o_{i}$ is the $i^{th}$ output unit from the activation layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected Layer\n",
    "\n",
    "Typically, a modern neural network contains millions of perceptrons as the one shown in the previous image. Perceptrons interact in different configurations such as cascaded or parallel. In this part, we describe a fully connected layer configuration in a neural network which comprises multiple parallel perceptrons forming one layer.\n",
    "\n",
    "We extend the previous notation to describe a fully connected layer. Each layer in a fully connected network has a number of input/hidden/output units cascaded in parallel. Let us a define a single layer of the neural net as follows: <br>\n",
    "$m$ denotes the number of hidden units in a single layer $l$ whereas $n$ denotes the number of units in the previous layer $l-1$.\n",
    "$$u^{[l]}=\\theta^{[l]}o^{[l-1]}+b^{[l]}$$\n",
    "where $u^{[l]} \\in R^{m}$ is a m-dimensional vector pertaining to the hidden units of the $l^{th}$ layer of the neural network after applying linear operations. Similarly, $o^{[l-1]} \\in R^{n}$ is the n-dimensional output vector corresponding to the hidden units of the $(l-1)^{th}$ activation layer. $\\theta^{[l]} \\in R^{m \\times n}$ is the weight matrix of the $l^{th}$ layer where each row of $\\theta^{[l]}$ is analogous to $\\theta_{i}$ described in the previous section i.e. each row corresponds to one hidden unit of the $l^{th}$ layer. $b^{[l]} \\in R^{m}$ is the bias vector of the layer where each element of b pertains to one hidden unit of the $l^{th}$ layer. This is followed by element wise non-linear activation function $o^{[l]} = \\phi(u^{[l]})$.\n",
    "The whole operation can be summarized as,\n",
    "$$o^{[l]} = \\phi(\\theta^{[l]}o^{[l-1]}+b^{[l]}) $$\n",
    "where $o^{[l-1]}$ is the output of the previous layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A dropout layer is a regularization technique used in neural networks to reduce overfitting. During training, a dropout layer looks at each input unit and randomly decide if it will be dropped (set to zero) with some given probability $p$. The decision for each unit is made independently. Formally, given an input of shape $N \\times K$ (where $N$ is the number of data points and $K$ is the number of features), it samples from $\\text{Bernoulli}(p)$ for each unit, resulting in an output where approximately $pNK$ of the units are zero (in expectation). This forces the network to learn more robust and generalizable features, since it cannot rely too much on any particular input. During inference, the dropout layer is turned off, and the full network is used to make predictions.\n",
    "\n",
    "The dropout probability $p$ is a hyperparameter than can be tuned to adjust the strength of regularization. Setting $p=0$ is equivalent to no dropout.\n",
    "\n",
    "Note that the derivative of $\\text{dropout}(u)$ with respect to $u$ has the same shape as $u$. The values of the derivative depend on the random mask.\n",
    "\n",
    "Use [this](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html) as a reference for your implementation.\n",
    "\n",
    "Note that after applying the mask, we must scale the result by a factor of $1/(1-p)$. Why is this necessary?\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>\\_dropout</strong> function in <strong>NN.py</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_dropout\").test_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss is a widely used loss function in machine learning and deep learning, especially for classification tasks. It measures the dissimilarity between the predicted probability distribution and the true probability distribution of a classification problem. If it is closer to zero, the better the learnt function is.\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "For classification problems as in this exercise, we compute the loss as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "CE = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\left(y_{i} \\cdot log(\\hat{y_{i}})\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $y_{i}$ is the true label and $\\hat{y_{i}}$ is the estimated label.\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>cross_entropy_loss</strong> function in <strong>NN.py</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_loss\").test_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "_The architecture of our neural network._\n",
    "\n",
    "![Neural Network](data/images/nn.jpg)\n",
    "\n",
    "The above diagram shows the dimensions of the neural network you will implement, along with the relationships between the quantities. Note that the neural network consists of two hidden linear layers, each followed by a SiLU activation function. The logits outputted by the second hidden linear layer are then passed through the softmax function, which turns them into probability distributions over the 3 classes.\n",
    "\n",
    "Here is a helpful [guide](https://static.us.edusercontent.com/files/gznuqr6aWHD8dPhiusG2TG53) that walks through the matrix multiplication operations and shapes involved in a forward and backward pass.\n",
    "  \n",
    "  <font color='darkred'>\n",
    "<strong> Note: Implement drop out function only on the first hidden layer! </strong>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_forward_without_dropout\").test_forward_without_dropout()\n",
    "TestNN(\"test_forward\").test_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation: Update Weights and Compute Gradients\n",
    "\n",
    "After the forward pass, we do back propagation to update the weights and biases in the direction of the negative gradient of the loss function.\n",
    "\n",
    "### Update Weights\n",
    "\n",
    "So, we update the weights and biases using the following formulas\n",
    "\\begin{align*}\n",
    "\\theta^{[3]} := \\theta^{[3]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[3]}} \\\\\n",
    "b^{[3]} := b^{[3]} - lr \\times \\frac{\\partial l}{\\partial b^{[3]}} \\\\\n",
    "\\theta^{[2]} := \\theta^{[2]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[2]}} \\\\\n",
    "b^{[2]} := b^{[2]} - lr \\times \\frac{\\partial l}{\\partial b^{[2]}} \\\\\n",
    "\\theta^{[1]} := \\theta^{[1]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[1]}} \\\\\n",
    "b^{[1]} := b^{[1]} - lr \\times \\frac{\\partial l}{\\partial b^{[1]}}\n",
    "\\end{align*}\n",
    "where $lr$ is the learning rate. It decides the step size we want to take in the direction of the negative gradient.\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>update_weights</strong> function in <strong>NN.py</strong> with use_momentum=False.\n",
    "\n",
    "Hint: Refer to this [guide](https://static.us.edusercontent.com/files/gznuqr6aWHD8dPhiusG2TG53) for more detail on the backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_update_weights\").test_update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Weights with Momentum [Bonus for Undergrad]\n",
    "\n",
    "Gradient descent does a generally good job of facilitating the convergence of the model's parameters to minimize the loss function. However, the process of doing so can be slow and/or noisy. **Momentum** is a technique used to stabilize this convergence.\n",
    "\n",
    "As a reminder, vanilla gradient descent applies the following update function to the parameters:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\theta_t$ represents the parameters at time $t$, $\\alpha$ represents the learning rate, and $f$ is the loss function.\n",
    "\n",
    "Momentum proposes the following tweak to our parameter update function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_{t+1} &= \\beta z_t + \\nabla f(\\theta_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha z_{t+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\beta \\in [0, 1]$ is the momentum constant and $z_t$ represents the momentum records at time $t$.\n",
    "\n",
    "You can think of momentum as taking our previous changes into consideration. If we've been moving in a certain direction recently, it's likely we should keep moving in that direction. The recurrence relation given shows that we use an exponentially-weighted average of the previous updates for our current update.\n",
    "\n",
    "A useful analogy about momentum from [this great article on Distill](https://distill.pub/2017/momentum/):\n",
    "\n",
    "> Hereâ€™s a popular story about momentum: gradient descent is a man walking down a hill. He follows the steepest path downwards; his progress is slow, but steady. Momentum is a heavy ball rolling down the same hill. The added inertia acts both as a smoother and an accelerator, dampening oscillations and causing us to barrel through narrow valleys, small humps and local minima.\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>update_weights</strong> function in <strong>NN.py</strong> with use_momentum=True.\n",
    "\n",
    "**HINT**: $z$ is stored in `self.change`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_update_weights_with_momentum\").test_update_weights_with_momentum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradients of the loss with respect to each parameter, we use the equations that make up the forward pass:\n",
    "\\begin{align*}\n",
    "u_1 &= \\theta_1 X + b_1 \\\\\n",
    "o_1 &= \\text{silu}(u_1) \\\\\n",
    "u_2 &= \\theta_2 o_1 + b_2 \\\\\n",
    "o_2 &= \\text{silu}(u_2)\\\\\n",
    "u_3 &= \\theta_3 o2 + b_3\\\\\n",
    "o_3 &= \\text{softmax}(u_3) \\\\\n",
    "l &= \\text{cross\\_entropy}(o_3)\n",
    "\\end{align*}\n",
    "\n",
    "When computing gradients, we travel backwards from the loss all the way back ot the input. We first seek to obtain the derivative of the loss $l$ with respect to the logits $u_3$. Note that they have the relation $$ l = \\text{cross\\_entropy}(\\text{softmax}(u_3))$$ Computing the derivative of this seems very involved, but it actually has a very elegant result: $$ \\frac{\\partial l}{\\partial u_3} = \\text{softmax}(u_3) - y = o_3 - y = \\hat{y} - y. $$ where $\\hat{y}$ is predicted y or $o_3$. \n",
    "\n",
    "While this is given to you, we encourage you to derive it for yourself! You can find a great explanation of the derivation [in this article](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1).\n",
    "\n",
    "Now that we have $\\frac{\\partial l}{\\partial u_3}$, we seek to move further back and compute $\\frac{\\partial l}{\\partial \\theta_3}$ and $\\frac{\\partial l}{\\partial b_3}$. This is done using the chain rule:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\theta_3} &= \\frac{\\partial l}{\\partial u_3} \\cdot \\frac{\\partial u_3}{\\partial \\theta_3} \\\\\n",
    "\\frac{\\partial l}{\\partial b_3} &= \\frac{\\partial l}{\\partial u_3} \\cdot \\frac{\\partial u_3}{\\partial b_3}.\n",
    "\\end{align*}\n",
    "\n",
    "The quantities $\\frac{\\partial u_3}{\\partial \\theta_3}$ and $\\frac{\\partial u_3}{\\partial b_3}$ are easy to derive from the relation $u_3 = \\theta_3 o_2 + b_3$. We see that\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\theta_3} &= \\frac{\\partial l}{\\partial u_3} \\cdot o_2 \\\\\n",
    "\\frac{\\partial l}{\\partial b_3} &= \\frac{\\partial l}{\\partial u_3} \\cdot 1.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the derivative involves $o_2$, which we computed during the forward pass. Fortunately, we saved that value in `self.cache`, so we don't need to compute it again!\n",
    "\n",
    "The same procedure is repeated to obtain the gradients for the upstream parameters $\\theta_2$ and $b_2$. We must first perform the intermediate steps of computing the derivative of the loss with respect to $o_2$ and then $u_2$. These are given by\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial o_2} &= \\frac{\\partial l}{\\partial u_3} \\cdot \\theta_3 \\\\\n",
    "\\frac{\\partial l}{\\partial u_2} &= \\frac{\\partial l}{\\partial o_2} \\cdot \\frac{\\partial\\,\\text{SiLu}}{\\partial u_2}.\n",
    "\\end{align*}\n",
    "\n",
    "The same procedure is repeated to obtain the gradients for the upstream parameters $\\theta_1$ and $b_1$. We must first perform the intermediate steps of computing the derivative of the loss with respect to $o_1$ and then $u_1$. These are given by\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial o_1} &= \\frac{\\partial l}{\\partial u_2} \\cdot \\theta_2 \\\\\n",
    "\\frac{\\partial l}{\\partial u_1} &= \\frac{\\partial l}{\\partial o_1} \\cdot \\frac{\\partial\\,\\text{SiLu}}{\\partial u_1}.\n",
    "\\end{align*}\n",
    "\n",
    "In the second relation, we must consider our use of dropout! If we applied dropout on a particular neuron, it should not be adjusted. To account for this, in the case of `use_dropout=True`, we must instead use $$ \\frac{\\partial l}{\\partial u_1} = \\frac{\\partial l}{\\partial o_1} \\cdot \\frac{\\partial\\,\\text{SiLu}}{\\partial u_1} \\cdot \\text{dropout\\_mask} \\cdot \\frac{1}{1-p}, $$ where $1 / (1-p)$ is the scaling factor and dropout_mask is stored in `self.cache`.\n",
    "\n",
    "The final step! We can use these values to compute the gradients for $\\theta_1$ and $b_1$, using the relation $u_1 = \\theta_1 X + b_1$, which are given by\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\theta_1} &= \\frac{\\partial l}{\\partial u_1} \\cdot X \\\\\n",
    "\\frac{\\partial l}{\\partial b_1} &= \\frac{\\partial l}{\\partial u_1} \\cdot 1.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Tips\n",
    "\n",
    "The above equations are given in matrix notation. When implementing these computations in code, the easiest way to make sure you are calculating the values correctly and in the right order is to check shapes. Any time you are doing a matrix/vector operation in NumPy, **check the shapes**.\n",
    "\n",
    "Since we are computing these gradients over $N$ data points, we must divide the gradients by $N$ to take the _average_ gradient. Make sure you are dividing by $N$ exactly once, no more and no less!\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>compute_gradients</strong> function in <strong>NN.py</strong>.\n",
    "\n",
    "<strong> Note: Implement drop out function only on the first hidden layer! </strong>\n",
    "\n",
    "Hint: Refer to this [guide](https://static.us.edusercontent.com/files/gznuqr6aWHD8dPhiusG2TG53) for more detail on computing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\n",
    "    \"test_compute_gradients_without_dropout\"\n",
    ").test_compute_gradients_without_dropout()\n",
    "TestNN(\"test_compute_gradients\").test_compute_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Local Test: Gradient Descent\n",
    "\n",
    "You may test your implementation of the GD function contained in **NN.py** in the cell below. See [Using the Local Tests](#using_local_tests) for more details. Look at the function documentation in gradient_descent for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_gradient_descent\").test_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4B3KRmadPzh"
   },
   "source": [
    "### 1.1.2 Local Test: Batch Gradient Descent [No Points]\n",
    "\n",
    "You may test your implementation of the BGD function contained in **NN.py** in the cell below. See [Using the Local Tests](#using_local_tests) for more details. Look at the function documentation in gradient_descent for guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6NPnjiDdPzi",
    "outputId": "f70e8ae7-54e1-450b-ceae-ad914a708b14"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_batch_gradient_descent\").test_batch_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Local Test: Gradient Descent with Momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may test your implementation of the GD function with momentum contained in **NN.py** in the cell below. See [Using the Local Tests](#using_local_tests) for more details. Revisit your implementation for update_weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN(\"test_gradient_descent_with_momentum\").test_gradient_descent_with_momentum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B79i8H2TMnPA"
   },
   "source": [
    "## 1.2 Loss plot and cross-entropy(CE) value for NN with Gradient Descent [5pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "Train your neural network implementation with gradient descent and print out the loss at every 1000th iteration (starting at iteration 0). The following cells will plot the loss vs epoch graph and calculate the final test cross-entropy(CE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nJudgS-MnPB"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from NN import NeuralNet\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_housing_dataset()\n",
    "\n",
    "nn = NeuralNet(\n",
    "    y_train, lr=0.01, use_dropout=False, use_momentum=False\n",
    ")  # initalize neural net class\n",
    "nn.gradient_descent(x_train, y_train, iter=60000)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = nn.predict(x_test)\n",
    "display_labels = [\"low\", \"med\", \"high\"]\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true, y_pred, normalize=\"true\", display_labels=display_labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f\"Training\")\n",
    "plt.xlabel(\"Epoch (thousands)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZLnsEBrMnPB",
    "outputId": "204445c4-5ae6-4a80-e900-5d451cc3c10d"
   },
   "outputs": [],
   "source": [
    "# Total loss\n",
    "y_hat = nn.forward(x_test, use_dropout=False)\n",
    "print(\"Cross entropy loss:\", round(nn.cross_entropy_loss(y_test, y_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4udQLmkMnPC"
   },
   "source": [
    "## 1.3 Loss plot and CE value for NN with BGD [5pts Grad / 0.7% Bonus for Undergrad] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "Train your neural network implementation with batch gradient descent and print out the loss at every 1000th iteration (starting at iteration 0). The following cells will plot the loss vs epoch graph and calculate the final test CE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVTSILdPMnPC",
    "outputId": "6416c1dc-0aad-4f65-f959-410ba0be5d45"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from NN import NeuralNet\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_housing_dataset()\n",
    "\n",
    "nn = NeuralNet(\n",
    "    y_train, lr=0.01, use_dropout=True, use_momentum=False\n",
    ")  # initalize neural net class\n",
    "nn.batch_gradient_descent(x_train, y_train, iter=60000, use_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f\"Training\")\n",
    "plt.xlabel(\"Epoch (1000)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = nn.predict(x_test)\n",
    "display_labels = [\"low\", \"med\", \"high\"]\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true, y_pred, normalize=\"true\", display_labels=display_labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ERh21q9MnPD",
    "outputId": "35d527c7-4f63-4c35-bd6b-c510c85aef0a"
   },
   "outputs": [],
   "source": [
    "# Total loss\n",
    "y_hat = nn.forward(x_test, use_dropout=False)\n",
    "print(\"Cross entropy loss:\", round(nn.cross_entropy_loss(y_test, y_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Loss plot and CE value for NN with Gradient Descent with Momentum [5pts Grad / 0.6% Bonus for Undergrad] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "Train your neural net implementation using gradient descent with momentum and print out the loss at every 1000th iteration (starting at iteration 0). The following cells will plot the loss vs epoch graph and calculate the final test CE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from NN import NeuralNet\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_housing_dataset()\n",
    "\n",
    "nn = NeuralNet(\n",
    "    y_train, lr=0.01, use_dropout=False, use_momentum=True\n",
    ")  # initalize neural net class\n",
    "nn.gradient_descent(x_train, y_train, iter=60000, use_momentum=True)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f\"Training\")\n",
    "plt.xlabel(\"Epoch (1000)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = nn.predict(x_test)\n",
    "display_labels = [\"low\", \"med\", \"high\"]\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true, y_pred, normalize=\"true\", display_labels=display_labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total loss\n",
    "y_hat = nn.forward(x_test, use_dropout=False)\n",
    "print(\"Cross entropy loss:\", round(nn.cross_entropy_loss(y_test, y_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# 2: Image Classification based on Convolutional Neural Networks [20pts: 20pts Grad / 2.7% Bonus for Undergrad + 1.1% Bonus for all] <span style=\"color:blue\">**[P]**</span><span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAcYa40pm86P"
   },
   "source": [
    "## 2.1 Image Classification using Pytorch and CNN [20pts Grad / 2.7% Bonus for Undergrad] <span style=\"color:blue\">**[P]**</span><span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "**Pytorch Description**\n",
    "\n",
    "[Pytorch](https://pytorch.org) is a Machine Learning/Deep Learning tensor library based on Python and Torch that uses dynamic computation graphs. Pytorch is used for applications using GPUs and CPUs.\n",
    "\n",
    "**Helpful Links**\n",
    "\n",
    "- [Install Pytorch](https://pytorch.org/get-started/locally/)\n",
    "- [Pytorch Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Pytorch**\n",
    "\n",
    "Make sure you installed pytorch and torchvision (directions [here](https://pytorch.org/get-started/locally/)).\n",
    "\n",
    "Please also see [Pytorch Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) to see how to load a data set, build a training loop, and test the model. Another good resource for building CNNs using Pytorch is [here](https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWSeZNmCm86P"
   },
   "source": [
    "### Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import non_deterministic\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5wmr3mt2yOu"
   },
   "source": [
    "### 2.1.1 Load FashionMNIST Dataset and Data Augmentation [5pts - Bonus for Undergrad]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxDuP6Yq2yOv"
   },
   "source": [
    "We use [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset to train our model. This is a dataset of 70,000 28x28 grayscale images in 10 classes. There are 60,000 training images and 10,000 test images. We provide code for you to download Fashion-MNIST dataset below.\n",
    "\n",
    "#### Data Augmentation [5pts]\n",
    "\n",
    "Data augmentation is a technique to increase the diversity of your training set by applying random (but realistic) transformations such as image rotation and flipping the image around an axis. If the dataset in a machine learning model is rich and sufficient, the model performs better and more accurately. We will preprocess the training and testing set, but only the training set will undergo augmentation.\n",
    "\n",
    "Go through the [Pytorch torchvision.transforms.v2 documentation](https://pytorch.org/vision/master/transforms.html) to see how to apply multiple transformations at once.\n",
    "\n",
    "In the <strong>cnn_image_transformations.py</strong> file, complete the following functions to understand the common practices used for preprocessing and augmenting the image data:\n",
    "\n",
    "- <strong>create_training_transformations</strong>\n",
    "\n",
    "  - In this function, you are going to preprocess and augment training data.\n",
    "\n",
    "    - PREPROCESS: Convert the given PIL Images to Tensors\n",
    "\n",
    "    - AUGMENTATION: Apply Random Horizontal Flip and Random Rotation\n",
    "\n",
    "- <strong>create_testing_transformations</strong>\n",
    "\n",
    "  - In this function, you are going to only preprocess testing data.\n",
    "\n",
    "    - PREPROCESS: Convert the given PIL Images to Tensors\n",
    "\n",
    "Please note that the Gradescope only checks if expected preprocessing layers are existent.\n",
    "\n",
    "**References**\n",
    "\n",
    "[v2.Compose()](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Compose.html)\n",
    "\n",
    "[v2.ToTensor()](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ToTensor.html) (Hint: Look at the warning)\n",
    "\n",
    "[v2.RandomHorizontalFlip()](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html)\n",
    "\n",
    "[v2.RandomApply()](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomApply.html)\n",
    "\n",
    "[v2.RandomRotation()](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html)\n",
    "\n",
    "[Article about performance regarding transformations](https://pytorch.org/vision/master/transforms.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zv8fRYkp2yOv",
    "outputId": "b943ca12-7c2f-470b-fc54-d03c4d25910d"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from cnn_image_transformations import (\n",
    "    create_testing_transformations,\n",
    "    create_training_transformations,\n",
    ")\n",
    "\n",
    "# Create Transformations\n",
    "training_transformations = create_training_transformations()\n",
    "testing_transformation = create_testing_transformations()\n",
    "\n",
    "# Load data\n",
    "trainset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=training_transformations\n",
    ")\n",
    "testset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=testing_transformation\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"Top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    ")\n",
    "\n",
    "print(trainset.data.shape)\n",
    "print(testset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IropaXmi2yOy"
   },
   "source": [
    "### Load some sample images from Fashion-MNIST [Setup - No points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "NaUQz7x52yOy",
    "outputId": "efcf12cc-5cc1-4c46-e779-d9b15165a678"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=32, shuffle=True, num_workers=2\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=32, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(\"Image size\")\n",
    "print(v2.functional.get_size(images[0]))\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJEjYyism86Y"
   },
   "source": [
    "As you can see from above, the FashionMNIST dataset contains different types of objects. The images have been size-normalized and objects remain centered in fixed-size images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deWtvYFKm86Z"
   },
   "source": [
    "### 2.1.2 Build convolutional neural network model [5pts Grad / 0.7% Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8ioOgl1m86Z"
   },
   "source": [
    "In this part, you need to build a convolutional neural network as described below. The architecture of the model is outlined.\n",
    "\n",
    "In the <strong>cnn.py</strong> file, complete the following functions:\n",
    "\n",
    "- <strong> \\_\\_init\\_\\_</strong>: See Defining Variables section\n",
    "- <strong>forward</strong>: See Defining Model section\n",
    "\n",
    "**[INPUT - CONV - CONV - MAXPOOL - DROPOUT - CONV - CONV - MAXPOOL - DROPOUT - AVERAGEPOOL - FC1 - DROPOUT - FC2 - DROPOUT - FC3]**\n",
    "\n",
    "> INPUT: [$28\\times28\\times1$] will hold the raw pixel values of the image, in this case, an image of width 28, height 28. This layer should give 8 filters and have appropriate padding to maintain shape.\n",
    "\n",
    "> CONV: Conv. layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to the input volume. In our example architecture, we decide to set the kernel_size to be $3\\times3$. For example, the output of the Conv. layer may look like $[28\\times28\\times8]$ if we set out_channels to be 8 and use appropriate paddings to maintain shape.\n",
    "\n",
    "> CONV: Additional Conv. layer take outputs from above layers and applies more filters. We set the kernel_size to be $3\\times3$ and out_channels to be 32.\n",
    "\n",
    "> MAXPOOL: MAXPOOL layer will perform a downsampling operation along the spatial dimensions (width, height). With pool size of $2\\times2$, resulting shape takes form $16\\times16$.\n",
    "\n",
    "> DROPOUT: DROPOUT layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> CONV: Additonal Conv. layer takes outputs from above layers and applies more filters. We set the kernel_size to be $3\\times3$ and out_channels to be 32. Appropriate paddings are used to maintain shape.\n",
    "\n",
    "> CONV: Additonal Conv. layer takes outputs from above layers and applies more filters. We set the kernel_size to be $3\\times3$ and out_channels to be 64. Appropriate paddings are used to maintain shape.\n",
    "\n",
    "> MAXPOOL: MAXPOOL layer will perform a downsampling operation along the spatial dimensions (width, height).\n",
    "\n",
    "> DROPOUT: Dropout layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> AVERAGEPOOL: AVERAGEPOOL layer will perform a downsampling operation along the spatial dimension (width, height). Checkout AdaptiveAvgPool2d below.\n",
    "\n",
    "> FC1: Dense layer which takes output from above layers, and has 256 neurons. Flatten() operations may be useful.\n",
    "\n",
    "> DROPOUT: Dropout layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> FC2: Dense layer which takes output from above layers, and has 128 neurons.\n",
    "\n",
    "> DROPOUT: Dropout layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> FC3: Dense layer with 10 neurons, and Softmax activation, is the final layer. The dimension of the output space is the number of classes.\n",
    "\n",
    "**Activation function**: Use LeakyReLU with negative_slope 0.01 as the activation function for Conv. layers and Dense layers unless otherwise indicated to build you model architecture\n",
    "\n",
    "Note that while this is a suggested model design, you may use other architectures and experiment with different layers for better results.\n",
    "\n",
    "The following links are Pytorch documentation for the layers you are going to use to build the CNN.\n",
    "\n",
    "- [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- [Dense](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [MaxPool](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "- [AdaptiveAvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)\n",
    "- [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "- [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html)\n",
    "- [Flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html)\n",
    "\n",
    "Lastly, if you would like to experiment with additional layers, explore the [torch.nn api](https://pytorch.org/docs/stable/nn.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "L6fLE8Wpm86Z",
    "outputId": "ccbd55e9-aba5-4337-9b72-028085f0967b"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Show the architecture of the model\n",
    "achi = plt.imread(\"./data/images/Architecture.png\")\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(achi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F5PWx17m86g"
   },
   "source": [
    "#### Defining model [5pts Grad / 0.7% Bonus for Undergrad]<span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s509lYZCMnPJ"
   },
   "source": [
    "You now need to complete the `__init__()` function and the `forward()` function in <strong>cnn.py</strong> to define your model structure.\n",
    "\n",
    "Your model is required to have at least 2 convolutional layers and at least 2 dense layers. Ensuring that these requirements are met will earn you 5pts.\n",
    "\n",
    "Once you have defined a model structure you may use the cell below to examine your architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKpTMi2rm86i",
    "outputId": "2014de42-de96-48da-df1e-697ea5d7b634"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# You can compare your architecture with the 'Architecture.png'\n",
    "\n",
    "from cnn import CNN\n",
    "\n",
    "net = CNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcCxKieHRk6L"
   },
   "source": [
    "### 2.1.3 Train the network [8pts Grad / 1% Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSPkbMIypNJ6"
   },
   "source": [
    "**Tuning:** Training the network is the next thing to try. You can set the hyperparameters in the cell below. If your hyperparameters are set properly, you should see the loss of the validation set decreased and the value of accuracy increased. <strong>It may take more than 15 minutes to train your model. </strong>\n",
    "\n",
    "- Recommended Batch Sizes fall in the range 32-512 (use powers of 2)\n",
    "\n",
    "- Recommended Epoch Counts fall in the range 5-20\n",
    "\n",
    "- Recommended Learning Rates fall in the range .0001-.01\n",
    "\n",
    "**Expected Result:** You should be able to achieve more than $90\\%$ accuracy on the test set to get full points. If you achieve accuracy between $75\\%$ to $84\\%$, you will only get 3 points. An accuracy between $84\\%$ to $90\\%$ will earn an additional 3pts.\n",
    "\n",
    "Note: If you would like to automate the tuning process, you can use a nested for loop to search for the hyperparameter that achieves the accuracy. You could also look into [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) for hyperparameter optimization.\n",
    "\n",
    "- $75\\%$ to $84\\%$ earns 3pts\n",
    "- $84\\%$ to $90\\%$ earns 3pts more (6pts total)\n",
    "- $90\\%$+ earns 2pts more (8pts total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXExabcOpAdI"
   },
   "source": [
    "#### Train your own CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeNII5h0EhBa",
    "outputId": "20c25acc-9f2e-4b86-9924-03d2eb545141"
   },
   "outputs": [],
   "source": [
    "from cnn import CNN\n",
    "from cnn_trainer import Trainer\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "# TODO: Change hyperparameters here\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "init_lr = 5e-3\n",
    "\n",
    "# Choose best device to speed up training\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    net,\n",
    "    trainset,\n",
    "    testset,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    init_lr=init_lr,\n",
    "    device=device,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv_HZge7MnPL"
   },
   "source": [
    "### 2.1.4 Examine accuracy and loss [2pts Grad / 0.3% Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkcOlCqDMnPL"
   },
   "source": [
    "You should expect to see gradually decreasing loss and gradually increasing accuracy. Examine loss and accuracy by running the cell below, no editing is necessary. Having appropriate looking loss and accuracy plots will earn you the last 2pts for your convolutional neural net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "w0dD-7Edm86o",
    "outputId": "85b9f1ed-fe71-4ad1-bd9d-22100dcdc9b7"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# list all data in history\n",
    "train_loss, train_accuracy, valid_loss, valid_accuracy = trainer.get_training_history()\n",
    "\n",
    "# summarize history for accuracy and loss\n",
    "plt.plot(train_accuracy)\n",
    "plt.plot(valid_accuracy)\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "67Sc8YnEMnPM",
    "outputId": "9ddba3c0-4334-427a-e1d1-e20a1ad70c20"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# make predictions\n",
    "y_pred, y_pred_classes, y_gt_classes = trainer.predict(testloader)\n",
    "y_pred_prob = torch.max(y_pred, dim=1).values\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(confusion_matrix(y_gt_classes, y_pred_classes))\n",
    "plt.title(\"Confusion matrix\", fontsize=16)\n",
    "plt.xticks(np.arange(10), classes, rotation=90, fontsize=12)\n",
    "plt.yticks(np.arange(10), classes, fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkn21kyUObtK"
   },
   "source": [
    "## 2.2 Exploring Deep CNN Architectures [1.1% Bonus for All] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhDboDvPObtK"
   },
   "source": [
    "The network you have produced is rather simple relative to many of those used in industry and research. Researchers have worked to make CNN models deeper and deeper over the past years in an effort to gain higher accuracy in predictions. While your model is only a handful of layers deep, some state of the art deep architectures may include up to 150 layers. However, this process has not been without challenges.\n",
    "\n",
    "One such problem is the problem of the exploding gradient. The initial weights assigned to the neural nets creating large losses. Big gradient values can accumulate to the point where large parameter updates are observed, causing gradient descents to oscillate without coming to global minima. Whatâ€™s even worse is that these parameters can be so large that they overflow and return NaN values that cannot be updated anymore. \n",
    "\n",
    "Many tactics have been used in an effort to solve this problem. One architecture, named Gradient Clipping, solves the vanishing gradient problem in a unique way. Researchers from Massachusetts Institute of Technology dicussed about the mechanism and a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. Take a moment to explore how Gradient Clipping tackles the vanishing gradient problem by reading the original research paper here: https://arxiv.org/pdf/1905.11881 (also included as PDF in papers directory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7JcSZtyObtK"
   },
   "source": [
    "**Question:** In your own words, explain how Gradient Clipping addresses the exploding gradient problem in 1-2 sentences below: (Please type answers directly in the cell below.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU_9UwLTObtK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FxKE3QN2yO6"
   },
   "source": [
    "# 3: Random Forests [40pts + 2.1% Bonus for All] <span style=\"color:blue\">**[P]**</span> <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "**NOTE**: Please use sklearn's ExtraTreeClassifier in your Random Forest implementation. [You can find more details about this classifier here.](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html)\n",
    "\n",
    "For context, the general difference between an extra tree and decision tree classifier is that the decision tree optimizes which feature to reduce entropy on and at what value to split, while an extra tree randomly splits on the features given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zP6DnH62yO9"
   },
   "source": [
    "## 3.1 Random Forest Implementation [35pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "The decision boundaries drawn by decision or extra trees are very sharp, and fitting a tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of an extra tree, we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging'). This stems from the idea that a collection of weak learners can learn decision boundaries as well as a strong learner. This is commonly called a Random Forest.\n",
    "\n",
    "We can build a Random Forest as a collection of extra trees, as follows:\n",
    "\n",
    "1. For every tree in the random forest, we're going to\n",
    "\n",
    "   a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset.\n",
    "\n",
    "   b) From the subsamples in part a, choose attributes at random without replacement to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (65% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "\n",
    "   c) Fit an extra tree to the subsample of data we've chosen to a certain depth.\n",
    "\n",
    "You can refresh your understanding with the lecture notes on random forests.\n",
    "\n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In the <strong>random_forest.py</strong> file, complete the following functions:\n",
    "\n",
    "- <strong>\\_bootstrapping</strong>: this function will be used in `bootstrapping()`\n",
    "- <strong>fit</strong>: Fit the extra trees initialized in `__init__` with the datasets created in `bootstrapping()`. You will need to call `bootstrapping()`.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "1. In the Random Forest Class, X is assumed to be a matrix with num_training rows and num_features columns where num_training is the number of total records and num_features is the number of features of each record. y is assumed to be a vector of labels of length num_training.\n",
    "2. Look out for TODO's for the parts that need to be implemented\n",
    "3. If you receive any `SettingWithCopyWarning` warnings from the Pandas library, you can safely ignore them.\n",
    "4. Hint: when bootstrapping, set replace = False while creating col_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8E74hGIm86t"
   },
   "source": [
    "## 3.2 Hyperparameter Tuning with a Random Forest [5pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In machine learning, hyperparameters are parameters that are set before the learning process begins. The max_depth, num_estimators, or max_features variables from 3.1 are examples of different hyperparameters for a random forest model. Let's first review the dataset in a bit more detail.\n",
    "\n",
    "#### Dataset Objective\n",
    "\n",
    "Imagine that we are a team of researchers working to track and document various information related to dry beans for a machine learning model that predicts what type of bean is represented. We know that there are multiple things to keep track of, such as the shapes and sizes that differentiate different types of beans. We will use the information we track and document in order to publish it for the general public.\n",
    "\n",
    "After much reflection within the research team, we come to the conclusion that we can use past observations on bean images to create a model.\n",
    "\n",
    "We will use our random forest algorithm from Q3.1 to predict the bean type.\n",
    "\n",
    "You can find more information on the dataset [here](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The barbunya bean, also known as the cranberry bean, was first bred in Colombia._\n",
    "\n",
    "![A barbunya bean](data/images/barbunya.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYwbegdZm86t"
   },
   "source": [
    "#### Loading the dataset\n",
    "\n",
    "The dataset that the company has collected has the following features:\n",
    "\n",
    "There were 16 features used in this dataset.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "1. Area: The area of a bean zone and the number of pixels within its boundaries\n",
    "2. Perimeter: Bean circumference is defined as the length of its border\n",
    "3. MajorAxisLength: The distance between the ends of the longest line that can be drawn from a bean\n",
    "4. MinorAxisLength: The longest line that can be drawn from the bean while standing perpendicular to the main axis\n",
    "5. AspectRatio: Defines the relationship between MajorAxisLength and MinorAxisLength\n",
    "6. Eccentricity: Eccentricity of the ellipse having the same moments as the region\n",
    "7. ConvexArea: Number of pixels in the smallest convex polygon that can contain the area of a bean seed\n",
    "8. EquivDiameter Equivalent diameter, the diameter of a circle having the same area as a bean seed area\n",
    "9. Extent Feature: The ratio of the pixels in the bounding box to the bean area\n",
    "10. Solidity: Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
    "11. Roundness: Calculated with the following formula: (4piA)/(P^2)\n",
    "12. Compactness: Measures the roundness of an object\n",
    "13. ShapeFactor1\n",
    "14. ShapeFactor2\n",
    "15. ShapeFactor3\n",
    "16. ShapeFactor4\n",
    "\n",
    "Output:\n",
    "\n",
    "17. Target value:\n",
    "    - Seker\n",
    "    - Barbunya\n",
    "    - Bombay\n",
    "    - Cali\n",
    "    - Dermosan\n",
    "    - Horoz\n",
    "    - Sira\n",
    "\n",
    "Your random forest model will try to predict this variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dry_bean_dataset = \"./data/Dry_Bean_Dataset.csv\"\n",
    "df = pd.read_csv(dry_bean_dataset)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "X = df.drop([\"Class\"], axis=1)\n",
    "y = label_encoder.fit_transform(df[\"Class\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "X_test = np.array(X_test)\n",
    "X_train, y_train, X_test, y_test = (\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    np.array(X_test),\n",
    "    np.array(y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uFNR6ikMnPO",
    "outputId": "ad4681bd-45f9-4b4a-ebab-ca1793c07822"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "assert X_train.shape == (9119, 16)\n",
    "assert y_train.shape == (9119,)\n",
    "assert X_test.shape == (4492, 16)\n",
    "assert y_test.shape == (4492,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlxXUpNE2yPA"
   },
   "source": [
    "In the following codeblock, train your random forest model with different values for max_depth, n_estimators, or max_features and evaluate each model on the held-out test set. Try to choose a combination of hyperparameters that maximizes your prediction accuracy on the test set (aim for 85%+).\n",
    "\n",
    "In **random_forest.py**, once you are satisfied with your chosen parameters, update the following function:\n",
    "\n",
    "- **select_hyperparameters**: change the values for `max_depth`, `n_estimators`, and `max_features`to your chosen values\n",
    "\n",
    "Submit this file to Gradescope. You must achieve at least a **85% accuracy** against the test set in Gradescope to receive full credit for this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from utilities.localtests import TestRandomForest\n",
    "\n",
    "\"\"\"\n",
    "Once you have implemented Random forest, you can run this cell. If you implemented _bootStrapping correctly,\n",
    "then this cell should execute without any errors.\n",
    "\"\"\"\n",
    "TestRandomForest(\"test_bootstrapping\").test_bootstrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "6n8GGVU7tYGh",
    "outputId": "4a83b962-d917-4a53-9dc8-2681735d9396"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many Extra trees are fitted for the random forest.\n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each extra tree.\n",
    "\n",
    "Tune these three parameters to achieve a better accuracy. n_estimators and max_depth must both\n",
    "be at least 3 in value for moderately reliable answers. While you can use the provided test set\n",
    "to evaluate your implementation, you will need to obtain 85% on the test set to receive full\n",
    "credit for this section.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.ensemble\n",
    "from random_forest import RandomForest\n",
    "from sklearn import preprocessing\n",
    "\n",
    "################# DO NOT CHANGE THIS RANDOM SEED ####################\n",
    "student_random_seed = 4641 + 7641\n",
    "#####################################################################\n",
    "\n",
    "################# CHANGE THESE VALUES ###############################\n",
    "\n",
    "n_estimators = 3  # Hint: Consider values between 3-15.\n",
    "max_depth = 3  # Hint: Consider values betweeen 3-15.\n",
    "max_features = 0.1  # Hint: Consider values betweeen 0.3-1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "random_forest = RandomForest(\n",
    "    n_estimators, max_depth, max_features, random_seed=student_random_seed\n",
    ")\n",
    "random_forest.fit(X_train, y_train)\n",
    "accuracy = random_forest.OOB_score(X_test, y_test)\n",
    "print(\"accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIQa-UIoMnPP"
   },
   "source": [
    "**DON'T FORGET**: Once you are satisfied with your chosen parameters, change the values for `max_depth`, `n_estimators`, and `max_features` in the `select_hyperparameters()` function of your RandomForest class in `random_forest.py` to your chosen values, and then submit this file to Gradescope. You must achieve at least a **85% accuracy** against the test set in Gradescope to receive full credit for this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a code block that plots a confusion matrix for the classifier's predictions on the test set. A few things to think about: What are some trends seen in the matrix? Why do they happen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "pred = random_forest.predict(X_test)\n",
    "labels = [\"Seker\", \"Barbunya\", \"Bombay\", \"Cali\", \"Horoz\", \"Sira\", \"Dermason\"]\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, pred, display_labels=labels, xticks_rotation=\"vertical\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ3Urx3Em86y"
   },
   "source": [
    "## 3.3 Plotting Feature Importance [1.1% Bonus for All] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "While building tree-based models, it's common to quantify how well splitting on a particular feature in an extra tree helps with predicting the target label in a dataset. Machine learning practitioners typically use \"Gini importance\", or the (normalized) total reduction in entropy brought by that feature to evaluate how important that feature is for predicting the target variable.\n",
    "\n",
    "Gini importance is typically calculated as the reduction in entropy from reaching a split in an extra tree weighted by the probability of reaching that split in the extra tree. Sklearn internally computes the probability for reaching a split by finding the total number of samples that reaches it during the training phase divided by the total number of samples in the dataset. This weighted value is our feature importance.\n",
    "\n",
    "Let's think about what this metric means with an example. A high probability of reaching a split on feature A in an extra tree trained on a dataset (many samples will reach this split for a decision) and a large reduction in entropy from splitting on feature A will result in a high feature importance value for feature A. This could mean feature A is a very important feature for predicting the probability of the target label. On the other hand, a low probability of reaching a split on feature B in an extra tree and a low reduction in entropy from splitting on feature B will result in a low feature importance value. This could mean feature B is not a very informative feature for predicting the target label. **Thus, the higher the feature importance value, the more important the feature is to predicting the target label.**\n",
    "\n",
    "Fortunately for us, fitting a sklearn.ExtraTreeClassifier to a dataset automatically computes the Gini importance for every feature in the extra tree and stores these values in a **feature_importances\\_** variable. [Review the docs for more details on how to access this variable](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier.feature_importances_)\n",
    "\n",
    "In the <strong>random_forest.py</strong> file, complete the following function:\n",
    "\n",
    "- <strong>plot_feature_importance</strong>: Make sure to sort the bars in descending order and remove any features with feature importance of 0\n",
    "\n",
    "In the cell below, call your implementation of `plot_feature_importance()` and display a bar plot that shows the feature importance values for at least one extra tree in your tuned random forest from Q3.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irV3hL6mm86z"
   },
   "outputs": [],
   "source": [
    "# TODO: Complete plot_feature_importance() in random_forest.py\n",
    "\n",
    "random_forest.plot_feature_importance(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0pMnninMnPV"
   },
   "source": [
    "Note that there isn't one \"correct\" answer here. We simply want you to investigate how different features in your random forest contribute to predicting the target variable.\n",
    "\n",
    "Also note that: the number of features can be different if you change max_features value since it ends up changing the number of features considered in bootstrapped datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 ADABoost [1% Bonus for All] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In lecture we learn how to implement bootstrapping, but there is another common method used to prevent overfitting which also incorporates multiple decision trees: boosting. For our implementation, boosting is where you assign importances to multiple models (weak learners) and return the result of their averaged output (a single strong learner). This is done sequentially, with importances being updated after the training of each new tree rather than in parallel like in bootstrapping\n",
    "\n",
    "Specifically, you will be implementing adaptive boosting (ADABoost) which reassigns importances to the randomized decision trees depending on their error weight.\n",
    "\n",
    "Additional Resource: For a detailed walkthrough of ADABoost implementation, see [Implementing the AdaBoost Algorithm from Scratch](https://www.geeksforgeeks.org/implementing-the-adaboost-algorithm-from-scratch/)\n",
    "\n",
    "Using their error weights you will recalculate the importances AKA `alpha` values. You can use this formula to do it per model:\n",
    "\n",
    "$$ \n",
    "\\alpha = \\frac{1}{2} \\log\\left(\\frac{1 - \\text{error}}{\\text{error} + 10^{-10}}\\right) \n",
    "$$\n",
    "\n",
    "Then you must update the error weights for all models for the next importance calculation by multiplying all weights using this formula:\n",
    "\n",
    "$$\n",
    "\\text{Updated Weights (Not Normalized)} = \\text{Weights} \\times e^{\\alpha \\times \\text{(Number of Incorrect Classifications)}}\n",
    "$$\n",
    "\n",
    "We give you predict_adaboost which returns the prediction from the result of all the trees. You must implement `adaboost()` which iterates over the number of estimators, training a new tree and then updating all trees' weights depending on its error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete adaboost() in random_forest.py\n",
    "\n",
    "TestRandomForest(\"test_adaboost\").test_adaboost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the accuracy of our model using the helper function `predict_adaboost()`! To pass Gradescope your accuracy should be above 85%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model = RandomForest(\n",
    "    n_estimators, max_depth, max_features, random_seed=student_random_seed\n",
    ")\n",
    "\n",
    "# Train the AdaBoost ensemble using adaboost method\n",
    "adaboost_model.adaboost(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set\n",
    "y_pred = adaboost_model.predict_adaboost(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"AdaBoost accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1udwVq0PVFz"
   },
   "source": [
    "# 4: SVM [15 pts] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG6jrvc_m861"
   },
   "source": [
    "Consider a dataset with the following points in two-dimensional space:\n",
    "\n",
    "| $$x_1$$ | $$x_2$$ | $$y$$ |\n",
    "| ------- | ------- | ----- |\n",
    "| -2.3    | 3       | -1    |\n",
    "| -1.5    | -1.5    | -1    |\n",
    "| 0.5     | 0.75    | -1    |\n",
    "| 1.9     | 2.25    | -1    |\n",
    "| -2.0    | 6.75    | 1     |\n",
    "| -1.1    | 6.0     | 1     |\n",
    "| 0.0     | 7.5     | 1     |\n",
    "| 1.5     | 6.0     | 1     |\n",
    "\n",
    "\n",
    "Here, $x_1$ and $x_2$ are features and $y$ is the label.\n",
    "\n",
    "Support Vector Machines (SVMs) aim to find a hyperplane that separates data points of different classes with the maximum margin. The larger the margin, the better the model can generalize to unseen data. Fortunately, scikit-learnâ€™s SVC class handles this computation for us programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fitting an SVM classifier [10 pts] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Fit the SVM Classifier [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the points are already linearly separable, determine the separating hyperplane using a linear SVM programatically. Record the weights (theta) and bias (intercept) terms for this separating hyperplane, rounded to three decimal places.\n",
    "\n",
    "Hint: To do this, you'll need to import the `SVC` class from the `sklearn.svm` module and initialize the SVC with a linear kernel. Then you can fit the data and find the separating hyperplane. Finally, you can get the needed values using `svm.coef_` and `svm.intercept_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Plot the SVM Classifier [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the features $x_1$â€‹ and $x_2$ using different colors to represent the labels $y$ (e.g., red for -1, blue for 1). Include the separating hyperplane on the plot. Make sure your plot clearly distinguishes the two classes and visually demonstrates the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using Kernels [5 pts] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we added another point, (0.8, -6.5) with label 1, which causes the dataset to be non-linearly separable. This can be seen by plotting the new set of points, which is impossible to split by a hyperplane that achieves 100% accuracy. To solve this issue, SVM uses kernels, which transform the points in a dataset to a higher-dimensional space making them linearly separable. In section 4.1, you used a linear kernel, which didn't transform the dataset since it was already linearly separable. \n",
    "\n",
    "Create a specific kernel (transformation function) that could be applied to the dataset with the new point that makes the data linearly separable again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Next Character Prediction using Recurrent Neural Networks (RNNs) [6.8% Bonus for All] <span style=\"color:green\">[W]</span> <span style=\"color:blue\">[P]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, weâ€™ll compare two foundational types of recurrent neural network architectures: Simple Recurrent Neural Networks (Simple RNNs) and Long Short-Term Memory networks (LSTMs). The goal is to train these models to generate text in the style of Macbeth by predicting the next character in a given sequence. This exercise will highlight how each architecture manages sequential dependencies in text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Recurrent Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks are a class of neural networks designed to handle sequential or time-series data, where the order of inputs matters. Unlike feedforward neural networks that treat each input independently, sequential networks maintain memory of previous inputs, making them ideal for tasks involving ordered data like text, time series, or video frames. These networks allow previous outputs to be used as inputs while having hidden states.\n",
    "\n",
    "Common applications include:\n",
    "- Text processing (language modeling, translation)\n",
    "- Machine translation (translating from one language to the other)\n",
    "- Time series prediction (stock prices, weather forecasting)\n",
    "\n",
    "### Types of Recurrent Neural Networks\n",
    "We can divide the applications of RNNs into four main categories:\n",
    "\n",
    "- **One-to-One**\n",
    "    - Takes a single input and produces a single output\n",
    "    - Like feedforward neural networks\n",
    "    - Example: Image classification (1 image â†’ 1 label)\n",
    "\n",
    "- **Many-to-One**\n",
    "    - Takes a sequence as input but produces a single output\n",
    "    - Example: Sentiment analysis (many words â†’ 1 sentiment)\n",
    "    - *This is what we'll be building for our character prediction task!*\n",
    "\n",
    "- **One-to-Many**\n",
    "    - Takes a single input and produces a sequence\n",
    "    - Example: Image captioning (1 image â†’ sequence of words)\n",
    "\n",
    "- **Many-to-Many**\n",
    "    - Takes a sequence and produces a sequence\n",
    "    - Examples:\n",
    "        - Translation (English sentence â†’ French sentence)\n",
    "        - Video frame prediction (sequence of frames â†’ next frames)\n",
    "\n",
    "Each type serves different purposes, and understanding which to use depends on your task's input and desired output structure.\n",
    "\n",
    "In this section, our goal will be a **many-to-one** task: predicting the next character given a sequence of characters. Now let's take a look at the models we can use to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Simple Recurrent Neural Networks\n",
    "\n",
    "Simple Recurrent Neural Networks (RNNs) are the simplest form of recurrent neural networks. They are similar to feedforward networks, but have connections that loop back on themselves, allowing information from previous steps to influence the current processing step. This looping mechanism enables Simple RNNs to capture dependencies over time, which is especially useful for tasks involving language, where the context of earlier words or characters affects the final prediction.\n",
    "Here's how Simple RNNs generally work:\n",
    "\n",
    "1. At each timestep, the model:\n",
    "   - Takes in a new input\n",
    "   - Uses its current hidden state (memory of previous inputs)\n",
    "   - Produces both an output and an updated hidden state\n",
    "\n",
    "2. This process repeats for each element in the sequence, with the hidden state carrying information forward.\n",
    "\n",
    "For our character prediction task, we'll use a many-to-one configuration:\n",
    "- Input: We feed in characters one at a time (like \"m\", \"a\", \"c\", \"b\", \"e\")\n",
    "- Hidden State: Gets updated with each new character\n",
    "- Output: We only use the final prediction to guess the next character (\"t\")\n",
    "\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "![intro_to_rnn](data/images/intro_to_rnn.webp)\n",
    "\n",
    "Simple RNNs often suffer from the vanishing gradient problem when performing backpropagation through time. This issue arises because the gradients are repeatedly multiplied by the weight matrix as they propagate through each time step. If the weights are small, these multiplications cause the gradients to decrease exponentially, effectively \"vanishing\" as they travel backward, while large weights can lead to gradient \"explosion.\" As a result, the network struggles to learn and retain long-term dependencies, meaning that early inputs in a sequence are gradually \"forgotten,\" and training becomes ineffective for tasks requiring the processing of lengthy sequences.\n",
    "\n",
    "For a detailed explanation of Simple RNNs and their limitations, see:\n",
    "- [An Introduction to Recurrent Neural Networks (RNNs)](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)\n",
    "- [The Vanishing Gradient Problem in RNNs](https://medium.com/metaor-artificial-intelligence/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22)\n",
    "- [Josh Starmer RNNs clearly explained](https://www.youtube.com/watch?v=AsNTP8Kwu80&pp=ygUOc3RhdHF1ZXN0IFJOTlM%3D)\n",
    "\n",
    "Note: Simple/Vanilla RNN and RNN are often used interchangeably since Simple RNNs represent the most basic implementation of RNNs. However, RNN is actually a broader term that encompasses any neural network model which incorporates hidden states and uses its previous outputs as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "Long Short-Term Memory networks (LSTMs) are an advanced type of recurrent neural network designed to better handle long sequences. Unlike Simple RNNs that have a single memory channel, LSTMs have two memory channels that allows the network to selectively remember or forget information over long sequences. \n",
    "At each timestep, an LSTM:\n",
    "   - Takes in a new input\n",
    "   - Uses two types of state:\n",
    "     * Cell State: Like a long-term memory that can preserve important information\n",
    "     * Hidden State: Like a working memory for immediate processing\n",
    "   - Uses three gates to control information flow:\n",
    "     * Forget gate: Decides what to remove from cell state\n",
    "     * Input gate: Decides what new information to store from the current input\n",
    "     * Output gate: Decides what parts of cell state to output\n",
    "\n",
    "\n",
    "This dual-memory system allows LSTMs to maintain important information for longer periods while still processing new inputs effectively. Think of it like reading a book while taking notes - the cell state is like your notebook where you write down important information, while the hidden state is like your immediate attention to the current word and its context.\n",
    "\n",
    "#### Architecture\n",
    "![intro_to_lstm](data/images/intro_to_lstm.png)\n",
    "\n",
    "For our character prediction task, we'll use a many-to-one LSTM configuration:\n",
    "- Input: Characters fed in one at a time (like \"m\", \"a\", \"c\", \"b\", \"e\") as input\n",
    "- Output: Get the prediced next character (\"t\") using the last hidden state\n",
    "\n",
    "\n",
    "Simple RNNs struggle with long sequences because they try to pass all information through a single channel that gets increasingly noisy over time. LSTMs solve this by having two channels and specialized gates that allow the network to selectively update its memory state, rather than forcing all information through a single transformation. The memory cell pathway solves the vanishing gradient problem by providing a direct route for gradients to propagate backwards through time steps without being repeatedly multiplied by small numbers that would cause them to vanish. The gating mechanisms ensure this pathway remains relatively stable while still allowing the network to selectively update its memory when needed.\n",
    "\n",
    "For a deeper dive into LSTMs and their gate mechanisms, refer to:\n",
    "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [The Role of Gates in LSTMs](https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537)\n",
    "- [Josh Starmer LSTM clearly explained](https://www.youtube.com/watch?v=YCzL96nL7j0&pp=ygUOc3RhdHF1ZXN0IFJOTlM%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Loading the text\n",
    "- We'll use Shakespeare's Macbeth from Project Gutenberg\n",
    "- The text is first standardized by:\n",
    "  - Converting to lowercase for consistency\n",
    "  - Standardizing line endings and spacing\n",
    "- This standardized text is referred to as the 'corpus' - our primary dataset for training and inference\n",
    "- We vectorize the text by treating every character in our text as an individual unit (e.g., 'macbeth' -> ['m', 'a', 'c'...])\n",
    "\n",
    "##### Character-to-Index Mapping\n",
    "   - Neural networks can't process raw text, so we need to convert characters to numbers. \n",
    "   - First, we find all the unique characters in our text (including alphabets and special characters like spaces and punctuations). This forms our vocabulary, whose size we call the vocabulary size\n",
    "   - Each character in the vocabulary receives a unique integer assignment using the default ASCII value\n",
    "   - We use a dictionary to store this mapping: {'a':1, 'b':2, 'c':3, ...}\n",
    "   - This mapping enables bidirectional conversion between characters and integers for model input and output interpretation\n",
    "   - We use this mapping to convert the vector of text into a vector of numerical values.\n",
    "   \n",
    "##### Index Representation\n",
    "   - You might be thinking that assigning numbers to letters based on alphabetical order seems rather arbitrary - and you'd be absolutely right! This is precisely why we need smarter index representations\n",
    "   - Using raw indices (like 1, 2, 3) implies an ordering and magnitude that doesn't exist\n",
    "   - For example, 'b' (2) is not \"twice as much\" as 'a' (1)\n",
    "   - We need a more sophisticated way to represent characters that avoids these misleading numerical relationships\n",
    "   \n",
    "      a) **One-Hot Encoding**: Convert index to binary vector\n",
    "        - Each index becomes a vector of zeros with a single 1 at the index of the character's value/index\n",
    "        - Example: \n",
    "          - index 1 ('a') â†’ [0,1,0,0,...,0] (length 40)\n",
    "          - index 5 ('e') â†’ [0,0,0,0,1,0,...,0] (length 40)\n",
    "        - Sparse (mostly zeros)\n",
    "        - No relationship between characters (both vowels 'a' and 'e' are completely different vectors)\n",
    "      \n",
    "      b) **Embeddings**: Convert index to learned dense vector\n",
    "        - Each character's index gets transformed into a list of numbers (a dense vector)\n",
    "        - Think of it like assigning each character a unique point a higher dimension coordinate plane\n",
    "        - The motivation is that we can position similar characters close to each other and transform discrete data like letters or words into a continous vector space that the model can use to make predictions from\n",
    "        - What makes embedding layers special is that they're learnable! The model figures out the best way to represent characters based on your specific data and task\n",
    "        - This is super useful since what makes characters \"similar\" changes between tasks - the model learns what similarity means for your specific case and groups characters accordingly\n",
    "        - The embedding layer's dimension parameter determines how many numbers we use to represent each character (the size of the vector space)\n",
    "        - Let's look at a real example with embedding dimension 4:\n",
    "          - The letter 'a' might become â†’ [0.2, -0.5, 0.1, 0.8]\n",
    "          - The letter 'e' might become â†’ [0.3, -0.4, 0.2, 0.7]\n",
    "        - In this example, model discovered that 'a' and 'e' are both vowels and represents them close to each other\n",
    "        - Additionally, embeddings are more memory efficient because they are dense (non-sparse) unlike one-hot vectors\n",
    "\n",
    "   - For our model, we will be using an embedding layer in the model for index representations. Since this is a learnable layer, it will be part of our model architecture and not pre-processing. \n",
    "\n",
    "\n",
    "#### Creating training data\n",
    "- When preparing text data, we need to break it into fixed-length sequences for training. Each sequence becomes a training example where we try to predict the next character. \n",
    "- Using a sliding window approach, we move through the text step by step. The sequence length (context window) is a key hyperparameter - too short and the model lacks context, too long and training becomes computationally intensive. For example, with text \"macbeth\" (context window=4):\n",
    "  ```\n",
    "  Window 1: \"macb\" â†’ predict \"e\"\n",
    "  Window 2: \"acbe \" â†’ predict \"t\"\n",
    "  Window 3: \"cbet\" â†’ predict \"h\"\n",
    "  ```\n",
    "- Our input data is size (`NUM_SEQUENCES`, `SEQUENCE_LEN`), where `NUM_SEQUENCES` is the number of sequences of size `SEQUENCE_LEN` in the text\n",
    "- Our target data is size (`NUM_SEQUENCES`, `1`) representing all the next characters for all sequences in the text\n",
    "- NOTE: While we show characters in the examples above for clarity, remember that the actual input and target data are numerical indices, not the characters themselves\n",
    "\n",
    "\n",
    "Please refer to <strong>preprocess_text_data</strong> located in utilities for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.utils import preprocess_text_data\n",
    "\n",
    "# load and preprocess text\n",
    "text = requests.get(\"https://www.gutenberg.org/files/1533/1533-0.txt\").text\n",
    "DATA = preprocess_text_data(text)\n",
    "\n",
    "# unpack processed data components\n",
    "X, Y, TEXT, CHAR_INDICES, INDICES_CHAR, VOCAB, VOCAB_SIZE, SEQUENCE_LEN = (\n",
    "    DATA[\"x\"],\n",
    "    DATA[\"y\"],\n",
    "    DATA[\"text\"],\n",
    "    DATA[\"char_indices\"],\n",
    "    DATA[\"indices_char\"],\n",
    "    DATA[\"vocab\"],\n",
    "    DATA[\"vocab_size\"],\n",
    "    DATA[\"sequence_len\"],\n",
    ")\n",
    "\n",
    "print(\"Length of Corpus: \", len(TEXT))\n",
    "print(\"Vocabulary Map: \", CHAR_INDICES)\n",
    "print(f\"Vocabulary Size: \", VOCAB_SIZE)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "Final preprocessed data includes:\n",
    "- <strong>X</strong>: Input sequences \n",
    "    - (shape: [`NUM_SEQUENCES`, `SEQUENCE_LEN`])\n",
    "    - Contains all character sequences of length SEQUENCE_LEN\n",
    "- <strong>Y</strong>: Target characters \n",
    "    - (shape: [`NUM_SEQUENCES`, `1`])\n",
    "    - Contains the next character that follows each sequence in X\n",
    "- <strong>VOCABULARY MAP</strong>: The mapping from all unique characters in the text and their numerical representations\n",
    "- <strong>VOCAB_SIZE</strong>: Total number of unique characters\n",
    "- <strong>SEQUENCE_LEN</strong>: Length of input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model Architecture [4.4% Bonus for All] <span style=\"color:blue\">[P]</span>\n",
    "\n",
    "Before diving into the specific architectures, let's understand how data shapes transform through the embedding layer.\n",
    "\n",
    "Input Sequence Shape Flow:\n",
    "1. Initial input: (BATCH_SIZE, SEQUENCE_LEN)\n",
    "   - BATCH_SIZE sequences containing SEQUENCE_LEN integers, where each integer represents a character from our vocabulary\n",
    "   - Example: If BATCH_SIZE=32 and SEQUENCE_LEN=15, shape is (32, 15)\n",
    "\n",
    "2. Embedding Layer: (BATCH_SIZE, SEQUENCE_LEN, EMBEDDING_DIM)\n",
    "   - Transforms each integer into a vector of size EMBEDDING_DIM\n",
    "   - Example: If EMBEDDING_DIM=64:\n",
    "     - Each character index becomes a vector of 64 numbers\n",
    "     - Shape expands from (32, 15) to (32, 15, 64)\n",
    "     - This means: 32 sequences, each 15 characters long, each character now represented by 64 numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Defining the Simple RNN Model [2.2% Bonus for All]\n",
    "\n",
    "In this part, you need to build a simple recurrent neural network using tensorflow. The architecture of the model is outlined below:\n",
    "\n",
    "![rnn_architecture](data/images/rnn_architecture.png)\n",
    "\n",
    "**[EMBEDDING - Simple RNN - FC - ACTIVATION]**\n",
    "> **EMBEDDING**: The Embedding layer maps each integer (representing a character) in the input sequence to a dense vector representation. Each character index becomes a vector of **embedding dimension**. It has an input dimension of **vocab_size** (the total number of unique characters or tokens) and an output dimension defined by **embedding_dim**. This transformation allows the model to capture semantic relationships in the data.\n",
    "> - Input shape: (batch_size, sequence_length) - A sequence of character indices\n",
    "> - Output shape: (batch_size, sequence_length, embedding_dim) - Each character transformed into an embedding vector\n",
    "\n",
    "> **Simple RNN**: This layer processes the sequence data, passing information through time steps to learn temporal patterns. It has **rnn_units** neurons, determining the model's ability to capture dependencies in the sequential data.\n",
    "> - Input shape: (batch_size, sequence_length, embedding_dim) - Sequence of embedding vectors\n",
    "> - Output shape: (batch_size, rnn_units) - Final state output\n",
    "\n",
    "> **FC (Dense Layer)**: A fully connected layer that transforms the RNN output to match the number of classes or possible output tokens. It has **vocab_size** neurons, ensuring that each output corresponds to a unique token or class.\n",
    "> - Input shape: (batch_size, rnn_units) - RNN final state\n",
    "> - Output shape: (batch_size, vocab_size) - Raw scores for each possible character\n",
    "\n",
    "> **ACTIVATION (Softmax)**: The softmax activation function is applied to the output layer, producing a probability distribution over the vocabulary, allowing the model to interpret each output as a confidence level for each class.\n",
    "> - Input shape: (batch_size, vocab_size) - Raw scores\n",
    "> - Output shape: (batch_size, vocab_size) - Probability distribution over characters\n",
    "\n",
    "\n",
    "You can refer to the following documentation on tensorflow layers for more details:\n",
    "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
    "- [Simple RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)\n",
    "- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "- [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>define_model</strong> function in <strong>rnn.py</strong>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from rnn import RNN\n",
    "\n",
    "rnn_model = RNN(VOCAB_SIZE, SEQUENCE_LEN)\n",
    "rnn_model.set_hyperparameters()\n",
    "rnn_model.define_model()\n",
    "rnn_model.build_model()\n",
    "rnn_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestRNN\n",
    "\n",
    "tester = TestRNN(\"test_rnn_architecture\").test_rnn_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Defining the LSTM Model [2.2% Bonus for All]\n",
    "\n",
    "In this part, you need to build a long short-term memory (LSTM) network as described below. The architecture of the model is outlined below:\n",
    "\n",
    "![lstm_architecture](data/images/lstm_architecture.png)\n",
    "\n",
    "**[EMBEDDING - LSTM - FC - ACTIVATION]**\n",
    "> **EMBEDDING**: The Embedding layer maps each integer in the input sequence to a dense vector representation. It has an input dimension of **vocab_size** (the total number of unique characters or tokens) and an output dimension defined by **embedding_dim**. This transformation allows the model to capture semantic relationships in the data.\n",
    "> - Input shape: (batch_size, sequence_length) - A sequence of character indices\n",
    "> - Output shape: (batch_size, sequence_length, embedding_dim) - Each character transformed into an embedding vector\n",
    "\n",
    "> **LSTM**: This layer processes the sequence data, passing information through time steps to learn temporal patterns. It has **lstm_units** neurons, determining the LSTM ability to capture dependencies in the sequential data.\n",
    "> - Input shape: (batch_size, sequence_length, embedding_dim) - Sequence of embedding vectors\n",
    "> - Output shape: (batch_size, lstm_units) - Final state output\n",
    "\n",
    "> **FC (Dense Layer)**: A fully connected layer that transforms the LSTM output to match the number of classes or possible output tokens. It has **vocab_size** neurons, ensuring that each output corresponds to a unique token or class.\n",
    "> - Input shape: (batch_size, lstm_units) - LSTM final state\n",
    "> - Output shape: (batch_size, vocab_size) - Raw scores for each possible character\n",
    "\n",
    "> **ACTIVATION (Softmax)**: The softmax activation function is applied to the output layer, producing a probability distribution over the vocabulary, allowing the model to interpret each output as a confidence level for each class.\n",
    "> - Input shape: (batch_size, vocab_size) - Raw scores\n",
    "> - Output shape: (batch_size, vocab_size) - Probability distribution over characters\n",
    "\n",
    "\n",
    "You can refer to the following documentation on tensorflow layers for more details:\n",
    "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
    "- [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "- [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>define_model</strong> function in <strong>lstm.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from lstm import LSTM\n",
    "\n",
    "lstm_model = LSTM(VOCAB_SIZE, SEQUENCE_LEN)\n",
    "lstm_model.set_hyperparameters()\n",
    "lstm_model.define_model()\n",
    "lstm_model.build_model()\n",
    "lstm_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestLSTM\n",
    "\n",
    "tester = TestLSTM(\"test_lstm_architecture\").test_lstm_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 RNN vs LSTM Model Text Generation Training Comparison Analysis [2.4% Bonus for All] <span style=\"color:green\">[W]</span>\n",
    "\n",
    "Next, we'll train our RNN and LSTM models with their respective hyperparameters for character-level text generation. The model architecture is designed to process fixed-length sequences and predict the next most likely character based on the learned patterns.\n",
    "\n",
    "During training, we need one final transformation of our target data. \n",
    "- As shown in both architectures, the model outputs a probability distribution over all possible characters (VOCAB_SIZE)\n",
    "- To compare this with our target data, we convert our target indices into one-hot vectors\n",
    "- For example, if our target is 'e' or 5 and VOCAB_SIZE is 34:\n",
    "  - Target index: `[5]` â†’ One-hot: `[0, 0, 0, 0, 1, 0, ..., 0]` (length 34)\n",
    "- This transformation happens in the training loop, converting Y from shape (BATCH_SIZE, 1) to (BATCH_SIZE, VOCAB_SIZE)\n",
    "- The categorical cross-entropy loss can then compare our predicted probability distribution with this one-hot target distribution\n",
    "\n",
    "\n",
    "#### Training Configuration\n",
    "- **Optimizer**: RMSprop (Root Mean Square Propagation) optimizer for efficient training of recurrent networks. Read more about it [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop).\n",
    "- **Loss Function**: Categorical crossentropy to measure accuracy of predicted probability distribution. Read more about it [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy).\n",
    "- **Training Parameters**: Number of epochs and batch size are defined in the hyperparameters section of `rnn.py` and `lstm.py` respectively\n",
    "\n",
    "The core training loop implementation can be found in `base_sequential_model.py`.\n",
    "\n",
    "NOTE: The initial model training may take 10-15 minutes per model. After that, the function will automatically load the saved weights stored in the `model_weights` directory, making subsequent runs much faster. If you want to retrain from scratch instead of using saved weights, you can either:\n",
    "- Set `train_from_scratch=True` in the parameters \n",
    "- Delete the existing weights from the `model_weights` directory\n",
    "\n",
    "#### Training Visualization\n",
    "To monitor the training progress:\n",
    "- A plot showing loss metrics across epochs will be generated\n",
    "- This helps identify potential overfitting or convergence issues\n",
    "- The visualization will be displayed automatically after training completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "rnn_model.train(x=X, y=Y, train_from_scratch=False)\n",
    "rnn_model.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "lstm_model.train(x=X, y=Y, train_from_scratch=False)\n",
    "lstm_model.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training both models, we can now use them to generate text in the style of Macbeth. The generation process involves three key components:\n",
    "\n",
    "1. Seed Selection\n",
    "   - Start with a random seed sequence from our text corpus\n",
    "   - This sequence provides initial context for the generation\n",
    "\n",
    "2. Prediction Loop\n",
    "   - Model predicts probability over all characters based on current sequence and we sample next character\n",
    "   - Append generated character to the sequence\n",
    "   - Slide window forward by one character to maintain context length and predict using generated character\n",
    "\n",
    "3. Temperature Parameter\n",
    "   - Controls the randomness of predictions:\n",
    "     - Lower values (< 0.5): More conservative, deterministic text\n",
    "     - Higher values (> 1.0): More diverse but potentially less coherent text\n",
    "     - Default value of 0.5 balances coherence and creativity\n",
    "\n",
    "Below, we use our TextGenerator class to generate samples from both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from text_generator import TextGenerator\n",
    "\n",
    "generator = TextGenerator(CHAR_INDICES, INDICES_CHAR, SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "start_index = random.randint(0, len(TEXT) - SEQUENCE_LEN - 1)\n",
    "seed_text = TEXT[start_index : start_index + SEQUENCE_LEN]\n",
    "\n",
    "generator.generate(model=rnn_model, seed_text=seed_text, length=150, temperature=0.75)\n",
    "generator.generate(model=lstm_model, seed_text=seed_text, length=150, temperature=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analyze your experience training Simple RNN and LSTM models for Macbeth character prediction and answer the following:\n",
    "\n",
    "1. Identify and explain a real-world application where Simple RNN would be more suitable than LSTM\n",
    "2. Identify and explain a real-world application where LSTM would be more suitable than Simple RNN\n",
    "\n",
    "For each case, support your answer using evidence from atleast 1 of these aspects from your training:\n",
    "\n",
    "A. Generated Text Quality\n",
    "- What patterns and quality differences did you observe in the text output?\n",
    "\n",
    "B. Training Duration\n",
    "- How long did the model take to train? \n",
    "\n",
    "C. Loss Convergence\n",
    "- How did the loss values change across training epochs?\n",
    "\n",
    "D. Final Loss Achieved\n",
    "- What was the final loss produced by the model?\n",
    "\n",
    "You can research and add other considerations to strengthen your argument, but ensure you include atleast 1 observed metric from your training. Some additional considerations include:\n",
    "- Inference speed\n",
    "- Memory requirements\n",
    "- Ability to maintain context\n",
    "- Simplicity of architecture\n",
    "\n",
    "You will receive full credit if you use evidence from training and make a sound argument about why one model would be more suitable than the other for both cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Response Format\n",
    "A. Simple RNN Best Use Case: [Application Name]\n",
    "- Choose 1+ observed metrics:\n",
    "   - [Metric name]: [What you observed in your training]\n",
    "- Why this matters: [Explain how your evidence supports using RNN over LSTM for this application]\n",
    "\n",
    "B. LSTM Best Use Case: [Application Name]\n",
    "- Choose 1+ observed metrics:\n",
    "   - [Metric name]: [What you observed in your training]\n",
    "- Why this matters: [Explain how your evidence supports using LSTM over RNN for this application]\n",
    "\n",
    "Optional, but recommended: Add theoretical considerations to strengthen your argument\n",
    "- [Theoretical difference & why it matters for your chosen application]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.stop()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SUMMER2022_HW4_Solution.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ml_hw4_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
